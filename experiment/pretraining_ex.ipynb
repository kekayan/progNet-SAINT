{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVvpNRQEbNyc",
        "outputId": "cb115f28-b830-446f-ae6a-580b342dc470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'progNet-SAINT'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 70 (delta 32), reused 52 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (70/70), 953.53 KiB | 12.22 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://kekayan:mytoken@github.com/kekayan/progNet-SAINT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YueZWRkbbQjV",
        "outputId": "173f9eb6-8429-44bd-d030-5e720e85dab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/progNet-SAINT/src\n",
            "mkdir: cannot create directory ‘output’: File exists\n"
          ]
        }
      ],
      "source": [
        "%cd progNet-SAINT/src/\n",
        "%mkdir output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CQ2OB2IDbbui"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "from utils import count_parameters, classification_scores, mean_sq_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "from models import SAINT\n",
        "from augmentations import embed_data_mask\n",
        "from augmentations import add_noise\n",
        "from pretraining import SAINT_pretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WlrXeEW5bdRF"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../data/clinical_and_other_features.csv\")\n",
        "df2 = pd.read_csv('../data/clinical_and_other_features_filtered.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VDrk7AqbhHv",
        "outputId": "1040950a-a687-4cf0-f5b5-71139dbada6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(610, 84) (312, 84)\n"
          ]
        }
      ],
      "source": [
        "df= df[df[\"Overall Near-complete Response:  Stricter Definition\"].isna()]\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "print(df.shape, df2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gw68_E4LbgKf"
      },
      "outputs": [],
      "source": [
        "opt_dict = {\n",
        "    'd_task': 'clf',\n",
        "    'dtask': 'clf',\n",
        "    'task': 'multiclass',\n",
        "    'batchsize': 32,\n",
        "    'pt_aug': ['mixup', 'cutmix'],\n",
        "    'pt_aug_lam': 0.1,\n",
        "    'pretrain_epochs': 250, #50\n",
        "    'nce_temp': 0.7,\n",
        "    'lam0': 0.5,\n",
        "    'lam1': 10,\n",
        "    'lam2': 1,\n",
        "    'lam3': 10,\n",
        "    'pt_projhead_style': 'diff',\n",
        "    'pt_tasks': ['contrastive','denoising'],\n",
        "    'mixup_lam': 0.3,\n",
        "    'ssl_samples': 312,\n",
        "    'lr':0.0001,\n",
        "    'train_noise_type':None,\n",
        "    'train_noise_level':0,\n",
        "    'save_path':\"./output/model.pt\"\n",
        "}\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "opt = AttributeDict(opt_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0FNhhtJEbnhJ"
      },
      "outputs": [],
      "source": [
        "def data_split(X,y,nan_mask,indices):\n",
        "    x_d = {\n",
        "        'data': X.values[indices],\n",
        "        'mask': nan_mask.values[indices]\n",
        "    }\n",
        "\n",
        "    if x_d['data'].shape != x_d['mask'].shape:\n",
        "        raise'Shape of data not same as that of nan mask!'\n",
        "\n",
        "    y_d = {\n",
        "        'data': y[indices].reshape(-1, 1)\n",
        "    }\n",
        "    return x_d, y_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "94KQSlIMbp5s"
      },
      "outputs": [],
      "source": [
        "class DataSetCatCon(Dataset):\n",
        "    def __init__(self, X, Y, cat_cols,task='clf',continuous_mean_std=None):\n",
        "\n",
        "        cat_cols = list(cat_cols)\n",
        "        X_mask =  X['mask'].copy()\n",
        "        X = X['data'].copy()\n",
        "        con_cols = list(set(np.arange(X.shape[1])) - set(cat_cols))\n",
        "        self.X1 = X[:,cat_cols].copy().astype(np.int64) #categorical columns\n",
        "        self.X2 = X[:,con_cols].copy().astype(np.float32) #numerical columns\n",
        "        self.X1_mask = X_mask[:,cat_cols].copy().astype(np.int64) #categorical columns\n",
        "        self.X2_mask = X_mask[:,con_cols].copy().astype(np.int64) #numerical columns\n",
        "        self.y = Y['data']#.astype(np.float32) if regression\n",
        "        self.cls = np.zeros_like(self.y,dtype=int)\n",
        "        self.cls_mask = np.ones_like(self.y,dtype=int)\n",
        "        if continuous_mean_std is not None:\n",
        "            mean, std = continuous_mean_std\n",
        "            self.X2 = (self.X2 - mean) / std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # X1 has categorical data, X2 has continuous\n",
        "        return np.concatenate((self.cls[idx], self.X1[idx])), self.X2[idx],self.y[idx], np.concatenate((self.cls_mask[idx], self.X1_mask[idx])), self.X2_mask[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qo3emPZ5brbe"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(df,p=[.65, .15, .2]):\n",
        "  df1 = df.drop(['Overall Near-complete Response:  Looser Definition','Near-complete Response (Graded Measure)'],axis=1)\n",
        "  df1.columns = df1.columns.str.strip()\n",
        "  pathologic_response_to_neoadjuvant_therapy = ['Pathologic response to Neoadjuvant therapy: Pathologic stage (T) following neoadjuvant therapy',\n",
        "        'Pathologic response to Neoadjuvant therapy:  Pathologic stage (N) following neoadjuvant therapy',\n",
        "        'Pathologic response to Neoadjuvant therapy:  Pathologic stage (M) following neoadjuvant therapy']\n",
        "  # df1.drop(pathologic_response_to_neoadjuvant_therapy, axis=1, inplace=True)\n",
        "  X = df1.drop('Overall Near-complete Response:  Stricter Definition',axis=1)\n",
        "  y = df1['Overall Near-complete Response:  Stricter Definition']\n",
        "  cont_columns = ['Date of Birth (Days)', 'Days to Surgery (from the date of diagnosis)', 'Age at last contact in EMR f/u(days)(from the date of diagnosis) ,last time patient known to be alive, unless age of death is reported(in such case the age of death',\n",
        "    'Age at mammo (days)', 'Days to distant recurrence(from the date of diagnosis)', 'Days to local recurrence (from the date of diagnosis)',\n",
        "    'Days to death (from the date of diagnosis)', 'Days to last local recurrence free assessment (from the date of diagnosis)',\n",
        "    ]\n",
        "  categorical_columns = list(set(X.columns) - set(cont_columns))\n",
        "\n",
        "  # convert categorical columns to str type\n",
        "  X[categorical_columns] = X[categorical_columns].astype(str)\n",
        "\n",
        "  cat_idxs = [X.columns.get_loc(c) for c in categorical_columns]\n",
        "  con_idxs = [X.columns.get_loc(c) for c in cont_columns]\n",
        "  X[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p = [.65, .15, .2], size=(X.shape[0],))\n",
        "\n",
        "  train_indices = X[X.Set==\"train\"].index\n",
        "  valid_indices = X[X.Set==\"valid\"].index\n",
        "  test_indices = X[X.Set==\"test\"].index\n",
        "\n",
        "  X = X.drop(columns=['Set'])\n",
        "  temp = X.fillna(\"MissingValue\")\n",
        "#   creates a bert style mask for the missing values\n",
        "  nan_mask = temp.ne(\"MissingValue\").astype(int)\n",
        "\n",
        "  cat_dims = []\n",
        "  for col in categorical_columns:\n",
        "      X[col] = X[col].fillna(\"MissingValue\")\n",
        "      l_enc = LabelEncoder()\n",
        "      X[col] = l_enc.fit_transform(X[col].values)\n",
        "      cat_dims.append(len(l_enc.classes_))\n",
        "\n",
        "  for col in cont_columns:\n",
        "      X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "      X.fillna(X.loc[train_indices, col].mean(), inplace=True)\n",
        "  y = y.values\n",
        "  l_enc = LabelEncoder()\n",
        "  y = l_enc.fit_transform(y)\n",
        "  X_train, y_train = data_split(X,y,nan_mask,train_indices)\n",
        "  X_valid, y_valid = data_split(X,y,nan_mask,valid_indices)\n",
        "  X_test, y_test = data_split(X,y,nan_mask,test_indices)\n",
        "  train_mean, train_std = np.array(X_train['data'][:,con_idxs],dtype=np.float32).mean(0), np.array(X_train['data'][:,con_idxs],dtype=np.float32).std(0)\n",
        "  train_std = np.where(train_std < 1e-6, 1e-6, train_std)\n",
        "  continuous_mean_std = np.array([train_mean,train_std]).astype(np.float32)\n",
        "  train_ds = DataSetCatCon(X_train, y_train, cat_idxs,'clf',continuous_mean_std)\n",
        "  trainloader = DataLoader(train_ds, batch_size=64, shuffle=True,num_workers=1)\n",
        "\n",
        "  valid_ds = DataSetCatCon(X_valid, y_valid, cat_idxs,'clf', continuous_mean_std)\n",
        "  validloader = DataLoader(valid_ds, batch_size=64, shuffle=False,num_workers=1)\n",
        "\n",
        "  test_ds = DataSetCatCon(X_test, y_test, cat_idxs,'clf', continuous_mean_std)\n",
        "  testloader = DataLoader(test_ds, batch_size=64, shuffle=False,num_workers=1)\n",
        "  y_dim = len(np.unique(y_train['data'][:,0]))\n",
        "  print('Number of classes in train:',y_dim)\n",
        "  # in test\n",
        "  print('Number of classes in test:',len(np.unique(y_test['data'][:,0]))\n",
        "  )\n",
        "  #in valid\n",
        "  print('Number of classes in valid:',len(np.unique(y_valid['data'][:,0]))\n",
        "  )\n",
        "\n",
        "\n",
        "  cat_dims = np.append(np.array([1]),np.array(cat_dims)).astype(int) #Appending 1 for CLS token, this is later used to generate embeddings.\n",
        "\n",
        "  return trainloader, validloader, testloader, cat_dims, con_idxs , cat_idxs, y_dim , continuous_mean_std , X_train, y_train, X_valid, y_valid, X_test, y_test, train_ds, valid_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFFxeRUib_i4",
        "outputId": "1b4e1024-3fee-47d0-be1a-6ee6cb9d97a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes in train: 1\n",
            "Number of classes in test: 1\n",
            "Number of classes in valid: 1\n"
          ]
        }
      ],
      "source": [
        "trainloader, validloader, testloader, cat_dims, con_idxs , cat_idxs, y_dim , continuous_mean_std, X_train, y_train, X_valid, y_valid, X_test, y_test, train_ds, valid_ds = prepare_dataset(df,[.8, .2, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XCaiWEtocBaz"
      },
      "outputs": [],
      "source": [
        "y_dim = 4 # ssl will have unlabelled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GLW1xeIdcOz7"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gjbsBAQVcMGy"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wPoQhhqucPO_",
        "outputId": "7b82b654-52d8-44a1-c4c6-32492efcdda6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SAINT(\n",
              "  (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
              "  (simple_MLP): ModuleList(\n",
              "    (0-7): 8 x simple_MLP(\n",
              "      (layers): Sequential(\n",
              "        (0): Linear(in_features=1, out_features=100, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=100, out_features=64, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transformer): RowColTransformer(\n",
              "    (embeds): Embedding(1108, 64)\n",
              "    (layers): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Residual(\n",
              "            (fn): Attention(\n",
              "              (to_qkv): Linear(in_features=64, out_features=384, bias=False)\n",
              "              (to_out): Linear(in_features=128, out_features=64, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Residual(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=64, out_features=512, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Dropout(p=0.8, inplace=False)\n",
              "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): PreNorm(\n",
              "          (norm): LayerNorm((5248,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Residual(\n",
              "            (fn): Attention(\n",
              "              (to_qkv): Linear(in_features=5248, out_features=1536, bias=False)\n",
              "              (to_out): Linear(in_features=512, out_features=5248, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): PreNorm(\n",
              "          (norm): LayerNorm((5248,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Residual(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=5248, out_features=41984, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Dropout(p=0.8, inplace=False)\n",
              "                (3): Linear(in_features=20992, out_features=5248, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (mask_embed): Embedding(82, 64)\n",
              "  )\n",
              "  (mlp): MLP(\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=5248, out_features=2624, bias=True)\n",
              "      (1): Linear(in_features=2624, out_features=1312, bias=True)\n",
              "      (2): Linear(in_features=1312, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (embeds): Embedding(1108, 64)\n",
              "  (mask_embeds_cat): Embedding(148, 64)\n",
              "  (mask_embeds_cont): Embedding(16, 64)\n",
              "  (single_mask): Embedding(2, 64)\n",
              "  (pos_encodings): Embedding(82, 64)\n",
              "  (mlp1): sep_MLP(\n",
              "    (layers): ModuleList(\n",
              "      (0): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (1): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (2): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (3): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (4): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (5): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=7, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (6): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (7): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (8): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=12, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (9): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (10): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (11): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (12): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=6, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (13): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=35, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (14): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (15): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (16): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (17-18): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=11, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (19): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (20): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=21, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (21): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (22): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (23): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (24): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=42, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (25): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (26): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (27): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (28): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (29): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (30): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (31): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (32): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=6, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (33-34): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (35): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (36): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (37): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=8, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (38): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (39): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (40-41): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=7, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (42): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (43): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (44): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (45): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=6, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (46): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=513, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (47-50): 4 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (51-52): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (53): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (54-55): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (56): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (57): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=186, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (58): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (59): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (60): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=35, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (61): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=21, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (62): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (63): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (64): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (65): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (66): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=12, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (67): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (68): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (69): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (70): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=6, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (71): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (72): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=6, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (73): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlp2): sep_MLP(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlpfory): simple_MLP(\n",
              "    (layers): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=1000, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=1000, out_features=4, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (pt_mlp): simple_MLP(\n",
              "    (layers): Sequential(\n",
              "      (0): Linear(in_features=5248, out_features=6297, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=6297, out_features=2624, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (pt_mlp2): simple_MLP(\n",
              "    (layers): Sequential(\n",
              "      (0): Linear(in_features=5248, out_features=6297, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=6297, out_features=2624, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SAINT(\n",
        "categories = tuple(cat_dims),\n",
        "num_continuous = len(con_idxs),\n",
        "dim = 64,              # embedding dimension\n",
        "dim_out = 1,\n",
        "depth = 1,             # depth of the network (nr. of transformer blocks)\n",
        "heads = 8,             # number of attention heads 8\n",
        "attn_dropout = 0.1,\n",
        "ff_dropout = 0.8,\n",
        "mlp_hidden_mults = (4, 2),\n",
        "cont_embeddings = 'MLP', # options: 'MLP', 'linear', 'hybrid' (MLP with continuous embeddings concatenated to the transformer block outputs)\n",
        "attentiontype = 'colrow', # options: 'col', 'row', 'colrow', 'colrowv2'\n",
        "final_mlp_style = 'sep',\n",
        "y_dim = y_dim\n",
        ")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpBUme5ScThO",
        "outputId": "3546cf19-37b8-49d9-fe2f-89c875dd136c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretraining begins!\n",
            "Model Checkpoint Saved!\n",
            "Epoch: 0, Running Loss: 197146.6640625 , Val Loss: 11548.0546875\n",
            "Early Stopping Counter:  1\n",
            "Epoch: 1, Running Loss: 170043.126953125 , Val Loss: 13673.0361328125\n",
            "Model Checkpoint Saved!\n",
            "Epoch: 2, Running Loss: 157035.1396484375 , Val Loss: 8272.951171875\n",
            "Model Checkpoint Saved!\n",
            "Epoch: 3, Running Loss: 118476.8837890625 , Val Loss: 5748.60888671875\n",
            "Early Stopping Counter:  1\n",
            "Epoch: 4, Running Loss: 105892.40869140625 , Val Loss: 11048.9619140625\n",
            "Early Stopping Counter:  2\n",
            "Epoch: 5, Running Loss: 95019.72082519531 , Val Loss: 6138.1435546875\n",
            "Early Stopping Counter:  3\n",
            "Epoch: 6, Running Loss: 84806.22998046875 , Val Loss: 6759.634765625\n",
            "Model Checkpoint Saved!\n",
            "Epoch: 7, Running Loss: 65827.77160644531 , Val Loss: 3806.14794921875\n",
            "Model Checkpoint Saved!\n",
            "Epoch: 8, Running Loss: 67376.71032714844 , Val Loss: 2984.6923828125\n",
            "Early Stopping Counter:  1\n",
            "Epoch: 9, Running Loss: 60294.553955078125 , Val Loss: 3557.9775390625\n",
            "Early Stopping Counter:  2\n",
            "Epoch: 10, Running Loss: 66989.53735351562 , Val Loss: 4047.731689453125\n",
            "Early Stopping Counter:  3\n",
            "Epoch: 11, Running Loss: 61807.77941894531 , Val Loss: 6057.51171875\n",
            "Early Stopping Counter:  4\n",
            "Epoch: 12, Running Loss: 61303.01495361328 , Val Loss: 7408.6875\n",
            "Early Stopping Counter:  5\n",
            "Epoch: 13, Running Loss: 67987.31225585938 , Val Loss: 3262.47314453125\n",
            "Early Stopping Counter:  6\n",
            "Epoch: 14, Running Loss: 47140.69577026367 , Val Loss: 4290.6962890625\n",
            "Model Checkpoint Saved!\n",
            "Epoch: 15, Running Loss: 57567.468994140625 , Val Loss: 1191.903076171875\n",
            "Early Stopping Counter:  1\n",
            "Epoch: 16, Running Loss: 44401.553771972656 , Val Loss: 1404.442138671875\n",
            "Model Checkpoint Saved!\n",
            "Epoch: 17, Running Loss: 34919.38055419922 , Val Loss: 417.8917236328125\n",
            "Early Stopping Counter:  1\n",
            "Epoch: 18, Running Loss: 26741.99301147461 , Val Loss: 2570.168212890625\n",
            "Early Stopping Counter:  2\n",
            "Epoch: 19, Running Loss: 33321.403869628906 , Val Loss: 995.5386962890625\n",
            "Early Stopping Counter:  3\n",
            "Epoch: 20, Running Loss: 21444.313354492188 , Val Loss: 1227.839111328125\n",
            "Early Stopping Counter:  4\n",
            "Epoch: 21, Running Loss: 40912.89538574219 , Val Loss: 1142.22314453125\n",
            "Early Stopping Counter:  5\n",
            "Epoch: 22, Running Loss: 25399.805786132812 , Val Loss: 2895.010498046875\n",
            "Early Stopping Counter:  6\n",
            "Epoch: 23, Running Loss: 25947.15380859375 , Val Loss: 1351.974365234375\n",
            "Early Stopping Counter:  7\n",
            "Epoch: 24, Running Loss: 28407.574096679688 , Val Loss: 1018.80615234375\n",
            "Early Stopping Counter:  8\n",
            "Epoch: 25, Running Loss: 18472.371368408203 , Val Loss: 1248.3564453125\n",
            "Early Stopping Counter:  9\n",
            "Epoch: 26, Running Loss: 20819.269653320312 , Val Loss: 3756.880126953125\n",
            "Early Stopping Counter:  10\n",
            "Epoch: 27, Running Loss: 33252.916076660156 , Val Loss: 470.171630859375\n",
            "Early Stopping Counter:  11\n",
            "Early Stopping!\n",
            "END OF PRETRAINING!\n"
          ]
        }
      ],
      "source": [
        "model = SAINT_pretrain(model,train_ds, valid_ds , opt, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xKMq7pXEcZtj"
      },
      "outputs": [],
      "source": [
        "# Labelled Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCD6J6Hrckpb",
        "outputId": "49bcbfc6-16de-401b-e335-ef4dbd8f4330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes in train: 4\n",
            "Number of classes in test: 4\n",
            "Number of classes in valid: 4\n"
          ]
        }
      ],
      "source": [
        "trainloader, validloader, testloader, cat_dims, con_idxs , cat_idxs, y_dim , continuous_mean_std, X_train, y_train, X_valid, y_valid, X_test, y_test,_,_ = prepare_dataset(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gBMfWniTclaa"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(model.parameters(),lr=0.0001, betas=(0.9,0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iw-NAceect9-"
      },
      "outputs": [],
      "source": [
        "modelsave_path='outputs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdhpA_4hcv1Y",
        "outputId": "dab9b146-3680-433a-e7cd-15d6dcc40fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are in semi-supervised learning case\n"
          ]
        }
      ],
      "source": [
        "print('We are in semi-supervised learning case')\n",
        "\n",
        "train_bsize = min(opt.ssl_samples//4,opt.batchsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj0sqSPdcx0Z",
        "outputId": "548fb3ef-414a-401b-d423-eaad3d0f540c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
        "trainloader = DataLoader(train_ds, batch_size=train_bsize, shuffle=True,num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsWgmn1iczZM"
      },
      "source": [
        "## Fine-tuning the pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kusKLlLic2QN",
        "outputId": "dcd3ff87-574c-485c-ada3-83ed5127ecb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training begins now for # 0 Fold.\n",
            "7.790730953216553\n",
            "[EPOCH 1] VALID ACCURACY: 67.347\n",
            "save model\n",
            "4.907882928848267\n",
            "3.9448465704917908\n",
            "3.151735633611679\n",
            "2.9420889914035797\n",
            "2.87822088599205\n",
            "[EPOCH 6] VALID ACCURACY: 83.673\n",
            "save model\n",
            "1.8738755285739899\n",
            "1.5066462010145187\n",
            "1.226074531674385\n",
            "1.4210545718669891\n",
            "1.2493825554847717\n",
            "[EPOCH 11] VALID ACCURACY: 85.714\n",
            "save model\n",
            "1.1248624213039875\n",
            "0.6137706339359283\n",
            "1.0559185519814491\n",
            "1.3972267657518387\n",
            "0.5240734964609146\n",
            "[EPOCH 16] VALID ACCURACY: 75.510\n",
            "0.3559200391173363\n",
            "0.6426158826798201\n",
            "0.5857284478843212\n",
            "0.4714122796431184\n",
            "0.6800737753510475\n",
            "[EPOCH 21] VALID ACCURACY: 89.796\n",
            "save model\n",
            "0.36392861790955067\n",
            "0.40189144713804126\n",
            "0.09753974201157689\n",
            "0.26842658687382936\n",
            "0.45418099220842123\n",
            "[EPOCH 26] VALID ACCURACY: 85.714\n",
            "0.7747304160147905\n",
            "0.7673263866454363\n",
            "0.4333154186606407\n",
            "1.1271958947181702\n",
            "0.467283196747303\n",
            "[EPOCH 31] VALID ACCURACY: 85.714\n",
            "0.5436786240898073\n",
            "0.2859296966344118\n",
            "0.23272510804235935\n",
            "0.49303268268704414\n",
            "0.14179989590775222\n",
            "[EPOCH 36] VALID ACCURACY: 87.755\n",
            "0.22161762788891792\n",
            "0.20079579716548324\n",
            "0.37087034340947866\n",
            "0.3453443883918226\n",
            "0.41304432920878753\n",
            "[EPOCH 41] VALID ACCURACY: 85.714\n",
            "0.17732942616567016\n",
            "0.2276942664757371\n",
            "0.4939216119237244\n",
            "0.17523178062401712\n",
            "0.6853108843788505\n",
            "[EPOCH 46] VALID ACCURACY: 85.714\n",
            "0.4349176324903965\n",
            "0.3212068867869675\n",
            "1.0276400297880173\n",
            "0.3524795094272122\n",
            "0.4659348148852587\n",
            "[EPOCH 51] VALID ACCURACY: 87.755\n",
            "0.7876506540924311\n",
            "0.37909748405218124\n",
            "0.8919190466403961\n",
            "0.21735827196971513\n",
            "0.39732719864696264\n",
            "[EPOCH 56] VALID ACCURACY: 87.755\n",
            "0.09436683991225436\n",
            "0.029730805370490998\n",
            "0.03879901103209704\n",
            "0.08412684238282964\n",
            "0.11675903592276882\n",
            "[EPOCH 61] VALID ACCURACY: 89.796\n",
            "0.022695820618537255\n",
            "0.17011438214831287\n",
            "0.17555830220226198\n",
            "0.09830643840541597\n",
            "0.10674379959891667\n",
            "[EPOCH 66] VALID ACCURACY: 85.714\n",
            "0.06002147336766939\n",
            "0.16927722247783095\n",
            "0.27810486675298307\n",
            "0.35901895991992205\n",
            "0.4828960292506963\n",
            "[EPOCH 71] VALID ACCURACY: 87.755\n",
            "0.1684190411333475\n",
            "0.04880070011859061\n",
            "0.5980116687132977\n",
            "0.28668830322567374\n",
            "0.18490545147142257\n",
            "[EPOCH 76] VALID ACCURACY: 89.796\n",
            "0.00907973175344523\n",
            "0.0420132870785892\n",
            "0.00922781351255253\n",
            "0.19916269059467595\n",
            "0.44815367890987545\n",
            "[EPOCH 81] VALID ACCURACY: 89.796\n",
            "0.5411094315350056\n",
            "0.16232643177499995\n",
            "0.3814918054267764\n",
            "0.31622158107347786\n",
            "0.10067170471484133\n",
            "[EPOCH 86] VALID ACCURACY: 89.796\n",
            "0.15432403263366723\n",
            "0.37866114920996097\n",
            "0.036576520930111656\n",
            "0.002087128678795125\n",
            "0.06998348415072542\n",
            "[EPOCH 91] VALID ACCURACY: 85.714\n",
            "0.218381039067026\n",
            "0.2746467516935809\n",
            "0.021927596899331547\n",
            "0.09577950194761797\n",
            "0.14280157556640916\n",
            "[EPOCH 96] VALID ACCURACY: 87.755\n",
            "0.033577310510736424\n",
            "0.007735407183645293\n",
            "0.18946650803627563\n",
            "0.03295601546415128\n",
            "0.1664024869678542\n",
            "[EPOCH 101] VALID ACCURACY: 87.755\n",
            "0.017026928815539577\n",
            "0.4134110680797676\n",
            "0.02853861459880136\n",
            "0.020241546619217843\n",
            "0.013228767667897046\n",
            "[EPOCH 106] VALID ACCURACY: 83.673\n",
            "0.017254450787731912\n",
            "0.3522208615904674\n",
            "0.0423669347219402\n",
            "0.20360930257083965\n",
            "0.12077351835614536\n",
            "[EPOCH 111] VALID ACCURACY: 91.837\n",
            "save model\n",
            "0.0077862573525635526\n",
            "0.002690418958081864\n",
            "0.07179109845708354\n",
            "0.09812402505122009\n",
            "0.08121666524675675\n",
            "[EPOCH 116] VALID ACCURACY: 83.673\n",
            "0.0027828800957649946\n",
            "0.1725273560587084\n",
            "0.027736103038478177\n",
            "0.02688289590878412\n",
            "0.009865214684396051\n",
            "[EPOCH 121] VALID ACCURACY: 89.796\n",
            "0.06616810251944116\n",
            "0.0846186478393065\n",
            "0.0007956749896029791\n",
            "0.1647371581990864\n",
            "0.002764214822192912\n",
            "[EPOCH 126] VALID ACCURACY: 87.755\n",
            "0.05875584380436294\n",
            "0.023787928111786982\n",
            "0.010785275497369184\n",
            "0.08938287597993622\n",
            "0.0010769188706944988\n",
            "[EPOCH 131] VALID ACCURACY: 87.755\n",
            "0.0013340826026251307\n",
            "0.005992281290673418\n",
            "0.19859679409546516\n",
            "0.008021326648304239\n",
            "0.0705922140259645\n",
            "[EPOCH 136] VALID ACCURACY: 85.714\n",
            "0.0008258614307123935\n",
            "0.0006240568036446348\n",
            "0.00043426455340522807\n",
            "0.0005906481123361118\n",
            "0.0035240351207903586\n",
            "[EPOCH 141] VALID ACCURACY: 87.755\n",
            "0.00013876970660930965\n",
            "0.0002006686413551506\n",
            "0.00038080933882156387\n",
            "0.0009941969219653402\n",
            "0.000502902476540612\n",
            "[EPOCH 146] VALID ACCURACY: 87.755\n",
            "0.00011134039550597663\n",
            "9.16736685212527e-05\n",
            "7.033276847323577e-05\n",
            "0.00022620307777287962\n",
            "7.353280238930893e-05\n",
            "[EPOCH 151] VALID ACCURACY: 87.755\n",
            "0.00020495309240686765\n",
            "4.102409980077937e-05\n",
            "0.02311922386297738\n",
            "0.011340993609337602\n",
            "0.003181486930770916\n",
            "[EPOCH 156] VALID ACCURACY: 83.673\n",
            "0.01199280671789893\n",
            "0.0002842890862666536\n",
            "6.064336400868342e-05\n",
            "0.0005883430696940195\n",
            "0.00037160706563099666\n",
            "[EPOCH 161] VALID ACCURACY: 87.755\n",
            "9.757687848832575e-05\n",
            "0.0009463605686050869\n",
            "2.1974819674142054e-05\n",
            "4.701765018921833e-05\n",
            "2.9842903359167394e-05\n",
            "[EPOCH 166] VALID ACCURACY: 87.755\n",
            "0.0002385344527056077\n",
            "7.22164293165406e-05\n",
            "2.6155996728505215e-05\n",
            "0.00012446495696849524\n",
            "9.75518327663849e-05\n",
            "[EPOCH 171] VALID ACCURACY: 87.755\n",
            "1.6569178910685878e-05\n",
            "9.294494191181002e-06\n",
            "1.633091733310721e-05\n",
            "2.888239288090233e-05\n",
            "8.149624602538097e-05\n",
            "[EPOCH 176] VALID ACCURACY: 87.755\n",
            "0.000656453203532692\n",
            "0.0004788674319797792\n",
            "0.00019339502605930647\n",
            "0.00010779398019167274\n",
            "5.6982141359185334e-05\n",
            "[EPOCH 181] VALID ACCURACY: 87.755\n",
            "2.8268423250210617e-05\n",
            "7.498788932025491e-05\n",
            "3.655008194414222e-05\n",
            "0.00012962055154730479\n",
            "5.122925074374507e-05\n",
            "[EPOCH 186] VALID ACCURACY: 87.755\n",
            "0.00016205865898655247\n",
            "0.00013889202594441485\n",
            "0.00020759367981781907\n",
            "0.0005576576210160056\n",
            "2.6101367893716088e-05\n",
            "[EPOCH 191] VALID ACCURACY: 87.755\n",
            "0.00015827850597815996\n",
            "0.00020301045236692516\n",
            "6.120550068544617e-06\n",
            "3.5088919162262755e-05\n",
            "1.35151418589885e-05\n",
            "[EPOCH 196] VALID ACCURACY: 87.755\n",
            "2.359420011543989e-05\n",
            "4.0959417617614235e-05\n",
            "1.2177680446257e-05\n",
            "1.504156153941949e-05\n",
            "0.0002038059528786107\n",
            "[EPOCH 201] VALID ACCURACY: 87.755\n",
            "4.178496666895626e-05\n",
            "5.231112891124212e-05\n",
            "4.3366039790271316e-05\n",
            "3.554760689894465e-05\n",
            "3.395092244318221e-05\n",
            "[EPOCH 206] VALID ACCURACY: 87.755\n",
            "6.229949065073015e-05\n",
            "5.614102406070742e-05\n",
            "2.6865324457503448e-05\n",
            "2.1076769712635723e-05\n",
            "9.974669978873862e-05\n",
            "[EPOCH 211] VALID ACCURACY: 87.755\n",
            "5.665716139446886e-05\n",
            "4.2366686301420486e-05\n",
            "1.610756436321026e-05\n",
            "0.0001813271226183133\n",
            "1.4487359578652104e-05\n",
            "[EPOCH 216] VALID ACCURACY: 87.755\n",
            "TEST ACCURACY: 88.312\n",
            "Training begins now for # 1 Fold.\n",
            "1.4867725223302841\n",
            "[EPOCH 1] VALID ACCURACY: 100.000\n",
            "save model\n",
            "0.6415715701878071\n",
            "0.9225185550749302\n",
            "0.9178532063961029\n",
            "1.1552273388952017\n",
            "2.6881712675094604\n",
            "[EPOCH 6] VALID ACCURACY: 100.000\n",
            "2.5020458102226257\n",
            "0.723894976079464\n",
            "3.314129351521842\n",
            "9.594885431230068\n",
            "4.261781275272369\n",
            "[EPOCH 11] VALID ACCURACY: 81.250\n",
            "2.26496921479702\n",
            "2.054335340857506\n",
            "2.092665545642376\n",
            "1.5424902392551303\n",
            "3.4301968812942505\n",
            "[EPOCH 16] VALID ACCURACY: 97.917\n",
            "2.362081825733185\n",
            "7.382054328918457\n",
            "9.459030881524086\n",
            "3.981273978948593\n",
            "4.366120144724846\n",
            "[EPOCH 21] VALID ACCURACY: 97.917\n",
            "9.099925696849823\n",
            "6.296347260475159\n",
            "8.64410400390625\n",
            "4.452966153443413\n",
            "7.441918164491653\n",
            "[EPOCH 26] VALID ACCURACY: 75.000\n",
            "9.142139494419098\n",
            "6.507276117801666\n",
            "3.3929784446954727\n",
            "4.370752289891243\n",
            "10.303143471479416\n",
            "[EPOCH 31] VALID ACCURACY: 93.750\n",
            "8.941296964883804\n",
            "6.570703834295273\n",
            "8.52848191591329\n",
            "6.758754223585129\n",
            "3.1529005989432335\n",
            "[EPOCH 36] VALID ACCURACY: 72.917\n",
            "2.276527523994446\n",
            "3.2121169976890087\n",
            "1.3070268076844513\n",
            "5.283470019698143\n",
            "2.652440296704299\n",
            "[EPOCH 41] VALID ACCURACY: 97.917\n",
            "4.177322238683701\n",
            "3.34120262414217\n",
            "1.0942385429516435\n",
            "1.262623966555111\n",
            "2.1663326025009155\n",
            "[EPOCH 46] VALID ACCURACY: 97.917\n",
            "4.3523887711399425\n",
            "1.7081276699900627\n",
            "1.9323435798287392\n",
            "1.2454691603779793\n",
            "1.7082476317882538\n",
            "[EPOCH 51] VALID ACCURACY: 95.833\n",
            "1.3613178106024861\n",
            "0.609825404593721\n",
            "0.945519644243177\n",
            "0.39899350702762604\n",
            "0.3176741616043728\n",
            "[EPOCH 56] VALID ACCURACY: 100.000\n",
            "0.6438684777176604\n",
            "0.784528462390881\n",
            "0.6385625321599946\n",
            "0.45217254059389234\n",
            "1.1090324535034597\n",
            "[EPOCH 61] VALID ACCURACY: 100.000\n",
            "0.44583345111459494\n",
            "0.24966873228549957\n",
            "0.14791910769417882\n",
            "0.3457414209842682\n",
            "0.47749928169650957\n",
            "[EPOCH 66] VALID ACCURACY: 97.917\n",
            "0.42599594651255757\n",
            "0.14732423424720764\n",
            "0.32872232561931014\n",
            "0.19924738630652428\n",
            "0.03790368148474954\n",
            "[EPOCH 71] VALID ACCURACY: 97.917\n",
            "0.32103567593730986\n",
            "0.32985460161580704\n",
            "0.2602976656053215\n",
            "0.24674238823354244\n",
            "0.32722486090096936\n",
            "[EPOCH 76] VALID ACCURACY: 93.750\n",
            "0.22295512024720665\n",
            "0.21407053833536338\n",
            "0.2401551390066743\n",
            "0.027969680726528168\n",
            "0.2386648587998934\n",
            "[EPOCH 81] VALID ACCURACY: 97.917\n",
            "0.1833541005034931\n",
            "0.25857942551374435\n",
            "0.19063016136351507\n",
            "0.1488493655633647\n",
            "0.40418575371586485\n",
            "[EPOCH 86] VALID ACCURACY: 97.917\n",
            "0.4402850191399921\n",
            "0.158405561640393\n",
            "0.17807981331134215\n",
            "0.14307330899100634\n",
            "0.0271556920488365\n",
            "[EPOCH 91] VALID ACCURACY: 97.917\n",
            "0.09627094928873703\n",
            "0.03661312864278443\n",
            "0.08317135460674763\n",
            "0.06331905783736147\n",
            "0.13095690292539075\n",
            "[EPOCH 96] VALID ACCURACY: 97.917\n",
            "0.056612310305354185\n",
            "0.015471119084395468\n",
            "0.6796966455876827\n",
            "0.11605531252098444\n",
            "0.03458047058666125\n",
            "[EPOCH 101] VALID ACCURACY: 97.917\n",
            "0.008861462527420372\n",
            "0.08473485658032587\n",
            "0.09680945530999452\n",
            "0.09238009793625679\n",
            "0.028704647527774796\n",
            "[EPOCH 106] VALID ACCURACY: 97.917\n",
            "TEST ACCURACY: 90.909\n",
            "Training begins now for # 2 Fold.\n",
            "0.0803977605428372\n",
            "[EPOCH 1] VALID ACCURACY: 100.000\n",
            "TEST ACCURACY: 89.610\n",
            "Training begins now for # 3 Fold.\n",
            "0.09005572879686952\n",
            "[EPOCH 1] VALID ACCURACY: 100.000\n",
            "TEST ACCURACY: 89.610\n",
            "Average best validation accuracy from all folds: 89.61039161682129\n"
          ]
        }
      ],
      "source": [
        "# Start K-Fold Cross Validation\n",
        "# Define the number of splits\n",
        "n_splits = 4\n",
        "best_valid_auroc = 0\n",
        "best_valid_accuracy = 0\n",
        "best_test_auroc = 0\n",
        "best_test_accuracy = 0\n",
        "best_valid_rmse = 100000\n",
        "\n",
        "early_stop_counter = 0\n",
        "early_stop_patience = 20\n",
        "\n",
        "# fold_dict = {}\n",
        "\n",
        "# Define the KFold object\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=21)\n",
        "\n",
        "# Initialize lists to store the train and validation indices for each fold\n",
        "train_indices_list = []\n",
        "valid_indices_list = []\n",
        "\n",
        "# Loop over the splits and get the train and validation indices for each fold\n",
        "for train_indices, valid_indices in kf.split(X_train['data']):\n",
        "    train_indices_list.append(train_indices)\n",
        "    valid_indices_list.append(valid_indices)\n",
        "best_test_accuracy_list = []\n",
        "# Loop over the folds and train the model on each fold\n",
        "for fold in range(n_splits):\n",
        "    # Get the train and validation indices for this fold\n",
        "    train_indices = train_indices_list[fold]\n",
        "    valid_indices = valid_indices_list[fold]\n",
        "\n",
        "    # Create the train and validation datasets and dataloaders for this fold\n",
        "    train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
        "    trainloader = DataLoader(train_ds, batch_size=train_bsize,num_workers=2, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n",
        "    valid_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
        "    validloader = DataLoader(valid_ds, batch_size=train_bsize, shuffle=False,num_workers=2, sampler=torch.utils.data.SubsetRandomSampler(valid_indices))\n",
        "    print(f'Training begins now for # {fold} Fold.')\n",
        "    # Train the model on this fold\n",
        "    for epoch in range(300):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            optimizer.zero_grad()\n",
        "            # x_categ is the the categorical data, with y appended as last feature. x_cont has continuous data. cat_mask is an array of ones same shape as x_categ except for last column(corresponding to y's) set to 0s. con_mask is an array of ones same shape as x_cont.\n",
        "            x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
        "            if opt.train_noise_type is not None and opt.train_noise_level>0:\n",
        "                noise_dict = {\n",
        "                    'noise_type' : opt.train_noise_type,\n",
        "                    'lambda' : opt.train_noise_level\n",
        "                }\n",
        "                if opt.train_noise_type == 'cutmix':\n",
        "                    x_categ, x_cont = add_noise(x_categ,x_cont, noise_params = noise_dict)\n",
        "                elif opt.train_noise_type == 'missing':\n",
        "                    cat_mask, con_mask = add_noise(cat_mask, con_mask, noise_params = noise_dict)\n",
        "            # We are converting the data to embeddings in the next step\n",
        "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model)\n",
        "            reps = model.transformer(x_categ_enc, x_cont_enc)\n",
        "            # select only the representations corresponding to y and apply mlp on it in the next step to get the predictions.\n",
        "            y_reps = reps[:,0,:]\n",
        "\n",
        "            y_outs = model.mlpfory(y_reps)\n",
        "            if opt.task == 'regression':\n",
        "                loss = criterion(y_outs,y_gts)\n",
        "            else:\n",
        "                loss = criterion(y_outs,y_gts.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(running_loss)\n",
        "        if epoch%5==0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                if opt.task in ['binary','multiclass']:\n",
        "                    accuracy, auroc = classification_scores(model, validloader, device, opt.task)\n",
        "                    # test_accuracy, test_auroc = classification_scores(model, testloader, device, opt.task)\n",
        "\n",
        "                    print('[EPOCH %d] VALID ACCURACY: %.3f' %\n",
        "                        (epoch + 1, accuracy ))\n",
        "                    # print('[EPOCH %d] TEST ACCURACY: %.3f' %\n",
        "                    #     (epoch + 1, test_accuracy ))\n",
        "\n",
        "            if opt.task =='multiclass':\n",
        "                if accuracy > best_valid_accuracy:\n",
        "                    best_valid_accuracy = accuracy\n",
        "                    early_stop_counter = 0\n",
        "                    print(\"save model\")\n",
        "                    torch.save({'model': model, 'state_dict': model.state_dict(),'optimizer' : optimizer.state_dict()},modelsave_path+f\"model-{fold}.pt\")\n",
        "                else:\n",
        "                  early_stop_counter +=1\n",
        "                  if early_stop_counter > early_stop_patience:\n",
        "                    break\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "            accuracy, auroc = classification_scores(model, testloader, device, opt.task)\n",
        "            print('TEST ACCURACY: %.3f' % accuracy)\n",
        "            best_test_accuracy_list.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# End K Fold\n",
        "# Calculate the average of the best accuracy from each fold\n",
        "average_best_valid_accuracy = sum(best_test_accuracy_list) / len(best_test_accuracy_list)\n",
        "print('Average best validation accuracy from all folds:', average_best_valid_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KYsY0MldMrh",
        "outputId": "c82dd5c7-76e3-45b8-9378-2eaed563c6ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array(88.31169, dtype=float32),\n",
              " array(90.909096, dtype=float32),\n",
              " array(89.61039, dtype=float32),\n",
              " array(89.61039, dtype=float32)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_test_accuracy_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "994t6G7Hc7_g"
      },
      "source": [
        "## Supervised Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TUvyPAOJc-en",
        "outputId": "be73309f-39b0-431e-f119-c0f2f109e21c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SAINT(\n",
              "  (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
              "  (simple_MLP): ModuleList(\n",
              "    (0-7): 8 x simple_MLP(\n",
              "      (layers): Sequential(\n",
              "        (0): Linear(in_features=1, out_features=100, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=100, out_features=64, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transformer): RowColTransformer(\n",
              "    (embeds): Embedding(828, 64)\n",
              "    (layers): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Residual(\n",
              "            (fn): Attention(\n",
              "              (to_qkv): Linear(in_features=64, out_features=384, bias=False)\n",
              "              (to_out): Linear(in_features=128, out_features=64, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Residual(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=64, out_features=512, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Dropout(p=0.8, inplace=False)\n",
              "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): PreNorm(\n",
              "          (norm): LayerNorm((5248,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Residual(\n",
              "            (fn): Attention(\n",
              "              (to_qkv): Linear(in_features=5248, out_features=1536, bias=False)\n",
              "              (to_out): Linear(in_features=512, out_features=5248, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): PreNorm(\n",
              "          (norm): LayerNorm((5248,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Residual(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=5248, out_features=41984, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Dropout(p=0.8, inplace=False)\n",
              "                (3): Linear(in_features=20992, out_features=5248, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (mask_embed): Embedding(82, 64)\n",
              "  )\n",
              "  (mlp): MLP(\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=5248, out_features=2624, bias=True)\n",
              "      (1): Linear(in_features=2624, out_features=1312, bias=True)\n",
              "      (2): Linear(in_features=1312, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (embeds): Embedding(828, 64)\n",
              "  (mask_embeds_cat): Embedding(148, 64)\n",
              "  (mask_embeds_cont): Embedding(16, 64)\n",
              "  (single_mask): Embedding(2, 64)\n",
              "  (pos_encodings): Embedding(82, 64)\n",
              "  (mlp1): sep_MLP(\n",
              "    (layers): ModuleList(\n",
              "      (0): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (1): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (2): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (3): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (4): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (5): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=9, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (6-7): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (8): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=11, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (9-10): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (11): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (12): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=6, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (13): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=40, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (14): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (15): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (16): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (17-18): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=12, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (19): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (20): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=27, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (21): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (22): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (23): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (24): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=27, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (25): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (26): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (27): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (28): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (29): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (30-31): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (32): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=6, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (33): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (34-35): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (36): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (37): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=9, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (38): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (39): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=6, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (40): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (41): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=9, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (42-43): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (44): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (45): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (46): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=275, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (47): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (48): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (49): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (50): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (51-52): 2 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (53-55): 3 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (56): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (57): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=115, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (58): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (59): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (60): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=40, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (61): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=27, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (62): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (63): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=4, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (64): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (65): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (66): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=11, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (67): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (68): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=8, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (69): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (70): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=7, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (71): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=3, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (72): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (73): simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlp2): sep_MLP(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x simple_MLP(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=320, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlpfory): simple_MLP(\n",
              "    (layers): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=1000, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=1000, out_features=4, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (pt_mlp): simple_MLP(\n",
              "    (layers): Sequential(\n",
              "      (0): Linear(in_features=5248, out_features=6297, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=6297, out_features=2624, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (pt_mlp2): simple_MLP(\n",
              "    (layers): Sequential(\n",
              "      (0): Linear(in_features=5248, out_features=6297, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=6297, out_features=2624, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SAINT(\n",
        "categories = tuple(cat_dims),\n",
        "num_continuous = len(con_idxs),\n",
        "dim = 64,              # embedding dimension\n",
        "dim_out = 1,\n",
        "depth = 1,             # depth of the network (nr. of transformer blocks)\n",
        "heads = 8,             # number of attention heads 8\n",
        "attn_dropout = 0.1,\n",
        "ff_dropout = 0.8,\n",
        "mlp_hidden_mults = (4, 2),\n",
        "cont_embeddings = 'MLP', # options: 'MLP', 'linear', 'hybrid' (MLP with continuous embeddings concatenated to the transformer block outputs)\n",
        "attentiontype = 'colrow', # options: 'col', 'row', 'colrow', 'colrowv2'\n",
        "final_mlp_style = 'sep',\n",
        "y_dim = y_dim\n",
        ")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVk_V8u9dDxP",
        "outputId": "63542838-e71a-40f5-d316-3669066a2a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training begins now for # 0 Fold.\n",
            "6.802564263343811\n",
            "[EPOCH 1] VALID ACCURACY: 73.469\n",
            "save model\n",
            "6.859379172325134\n",
            "6.740860104560852\n",
            "6.736995816230774\n",
            "6.774489402770996\n",
            "6.818942666053772\n",
            "[EPOCH 6] VALID ACCURACY: 73.469\n",
            "6.8574992418289185\n",
            "6.733899474143982\n",
            "6.728445887565613\n",
            "6.785537958145142\n",
            "6.800738096237183\n",
            "[EPOCH 11] VALID ACCURACY: 73.469\n",
            "6.744778275489807\n",
            "6.768271207809448\n",
            "6.742018342018127\n",
            "6.84459376335144\n",
            "6.808243274688721\n",
            "[EPOCH 16] VALID ACCURACY: 73.469\n",
            "6.74475371837616\n",
            "6.760308384895325\n",
            "6.765004515647888\n",
            "6.832910776138306\n",
            "6.780970215797424\n",
            "[EPOCH 21] VALID ACCURACY: 73.469\n",
            "6.7799776792526245\n",
            "6.782591223716736\n",
            "6.7229695320129395\n",
            "6.748477339744568\n",
            "6.761236906051636\n",
            "[EPOCH 26] VALID ACCURACY: 73.469\n",
            "6.8648316860198975\n",
            "6.731154203414917\n",
            "6.793287992477417\n",
            "6.75217592716217\n",
            "6.788585901260376\n",
            "[EPOCH 31] VALID ACCURACY: 73.469\n",
            "6.796419262886047\n",
            "6.770337343215942\n",
            "6.806878328323364\n",
            "6.75973904132843\n",
            "6.747180342674255\n",
            "[EPOCH 36] VALID ACCURACY: 73.469\n",
            "6.801271319389343\n",
            "6.801507234573364\n",
            "6.791974425315857\n",
            "6.751264452934265\n",
            "6.810216784477234\n",
            "[EPOCH 41] VALID ACCURACY: 73.469\n",
            "6.757580757141113\n",
            "6.780599594116211\n",
            "6.785017728805542\n",
            "6.804702997207642\n",
            "6.8220590353012085\n",
            "[EPOCH 46] VALID ACCURACY: 73.469\n",
            "6.791608810424805\n",
            "6.8410561084747314\n",
            "6.748546361923218\n",
            "6.771297931671143\n",
            "6.806496858596802\n",
            "[EPOCH 51] VALID ACCURACY: 73.469\n",
            "6.778723359107971\n",
            "6.72438383102417\n",
            "6.7709126472473145\n",
            "6.759033918380737\n",
            "6.746991872787476\n",
            "[EPOCH 56] VALID ACCURACY: 73.469\n",
            "6.732579827308655\n",
            "6.789391994476318\n",
            "6.811728000640869\n",
            "6.762344717979431\n",
            "6.828739047050476\n",
            "[EPOCH 61] VALID ACCURACY: 73.469\n",
            "6.704049110412598\n",
            "6.793952465057373\n",
            "6.860874176025391\n",
            "6.74658727645874\n",
            "6.765553832054138\n",
            "[EPOCH 66] VALID ACCURACY: 73.469\n",
            "6.755523681640625\n",
            "6.7240307331085205\n",
            "6.81633722782135\n",
            "6.743815779685974\n",
            "6.820582985877991\n",
            "[EPOCH 71] VALID ACCURACY: 73.469\n",
            "6.81565535068512\n",
            "6.790650725364685\n",
            "6.82083797454834\n",
            "6.831339240074158\n",
            "6.766643524169922\n",
            "[EPOCH 76] VALID ACCURACY: 73.469\n",
            "6.790795087814331\n",
            "6.860920190811157\n",
            "6.755747199058533\n",
            "6.769964933395386\n",
            "6.813325047492981\n",
            "[EPOCH 81] VALID ACCURACY: 73.469\n",
            "6.834726095199585\n",
            "6.780413508415222\n",
            "6.777055382728577\n",
            "6.803710341453552\n",
            "6.765033483505249\n",
            "[EPOCH 86] VALID ACCURACY: 73.469\n",
            "6.77882194519043\n",
            "6.798382878303528\n",
            "6.756268858909607\n",
            "6.799512028694153\n",
            "6.837964296340942\n",
            "[EPOCH 91] VALID ACCURACY: 73.469\n",
            "6.759804725646973\n",
            "6.804957270622253\n",
            "6.807815909385681\n",
            "6.774259090423584\n",
            "6.788443446159363\n",
            "[EPOCH 96] VALID ACCURACY: 73.469\n",
            "6.724154949188232\n",
            "6.760883569717407\n",
            "6.783701539039612\n",
            "6.761573076248169\n",
            "6.847790360450745\n",
            "[EPOCH 101] VALID ACCURACY: 73.469\n",
            "6.792632579803467\n",
            "6.749007701873779\n",
            "6.793019652366638\n",
            "6.793559312820435\n",
            "6.778220295906067\n",
            "[EPOCH 106] VALID ACCURACY: 73.469\n",
            "TEST ACCURACY: 62.338\n",
            "Training begins now for # 1 Fold.\n",
            "6.679256796836853\n",
            "[EPOCH 1] VALID ACCURACY: 64.583\n",
            "TEST ACCURACY: 62.338\n",
            "Training begins now for # 2 Fold.\n",
            "6.70847475528717\n",
            "[EPOCH 1] VALID ACCURACY: 60.417\n",
            "TEST ACCURACY: 62.338\n",
            "Training begins now for # 3 Fold.\n",
            "6.667062163352966\n",
            "[EPOCH 1] VALID ACCURACY: 60.417\n",
            "TEST ACCURACY: 62.338\n",
            "Average best validation accuracy from all folds: 62.33766174316406\n"
          ]
        }
      ],
      "source": [
        "# Start K-Fold for without pre-train\n",
        "# Define the number of splits\n",
        "n_splits = 4\n",
        "best_valid_auroc = 0\n",
        "best_valid_accuracy = 0\n",
        "best_test_auroc = 0\n",
        "best_test_accuracy = 0\n",
        "best_valid_rmse = 100000\n",
        "\n",
        "early_stop_counter = 0\n",
        "early_stop_patience = 20\n",
        "\n",
        "# fold_dict = {}\n",
        "\n",
        "# Define the KFold object\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=21)\n",
        "\n",
        "# Initialize lists to store the train and validation indices for each fold\n",
        "train_indices_list = []\n",
        "valid_indices_list = []\n",
        "\n",
        "# Loop over the splits and get the train and validation indices for each fold\n",
        "for train_indices, valid_indices in kf.split(X_train['data']):\n",
        "    train_indices_list.append(train_indices)\n",
        "    valid_indices_list.append(valid_indices)\n",
        "best_valid_accuracy_list = []\n",
        "# Loop over the folds and train the model on each fold\n",
        "for fold in range(n_splits):\n",
        "    # Get the train and validation indices for this fold\n",
        "    train_indices = train_indices_list[fold]\n",
        "    valid_indices = valid_indices_list[fold]\n",
        "\n",
        "    # Create the train and validation datasets and dataloaders for this fold\n",
        "    train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
        "    trainloader = DataLoader(train_ds, batch_size=train_bsize,num_workers=2, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n",
        "    valid_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
        "    validloader = DataLoader(valid_ds, batch_size=train_bsize, shuffle=False,num_workers=2, sampler=torch.utils.data.SubsetRandomSampler(valid_indices))\n",
        "    print(f'Training begins now for # {fold} Fold.')\n",
        "    # Train the model on this fold\n",
        "    for epoch in range(300):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            optimizer.zero_grad()\n",
        "            # x_categ is the the categorical data, with y appended as last feature. x_cont has continuous data. cat_mask is an array of ones same shape as x_categ except for last column(corresponding to y's) set to 0s. con_mask is an array of ones same shape as x_cont.\n",
        "            x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
        "            if opt.train_noise_type is not None and opt.train_noise_level>0:\n",
        "                noise_dict = {\n",
        "                    'noise_type' : opt.train_noise_type,\n",
        "                    'lambda' : opt.train_noise_level\n",
        "                }\n",
        "                if opt.train_noise_type == 'cutmix':\n",
        "                    x_categ, x_cont = add_noise(x_categ,x_cont, noise_params = noise_dict)\n",
        "                elif opt.train_noise_type == 'missing':\n",
        "                    cat_mask, con_mask = add_noise(cat_mask, con_mask, noise_params = noise_dict)\n",
        "            # We are converting the data to embeddings in the next step\n",
        "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model)\n",
        "            reps = model.transformer(x_categ_enc, x_cont_enc)\n",
        "            # select only the representations corresponding to y and apply mlp on it in the next step to get the predictions.\n",
        "            y_reps = reps[:,0,:]\n",
        "\n",
        "            y_outs = model.mlpfory(y_reps)\n",
        "            if opt.task == 'regression':\n",
        "                loss = criterion(y_outs,y_gts)\n",
        "            else:\n",
        "                loss = criterion(y_outs,y_gts.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(running_loss)\n",
        "        if epoch%5==0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                if opt.task in ['binary','multiclass']:\n",
        "                    accuracy, auroc = classification_scores(model, validloader, device, opt.task)\n",
        "                    # test_accuracy, test_auroc = classification_scores(model, testloader, device, opt.task)\n",
        "\n",
        "                    print('[EPOCH %d] VALID ACCURACY: %.3f' %\n",
        "                        (epoch + 1, accuracy ))\n",
        "                    # print('[EPOCH %d] TEST ACCURACY: %.3f' %\n",
        "                    #     (epoch + 1, test_accuracy ))\n",
        "\n",
        "            if opt.task =='multiclass':\n",
        "                if accuracy > best_valid_accuracy:\n",
        "                    best_valid_accuracy = accuracy\n",
        "                    early_stop_counter = 0\n",
        "                    print(\"save model\")\n",
        "                    torch.save({'model': model, 'state_dict': model.state_dict(),'optimizer' : optimizer.state_dict()},modelsave_path+f\"model-{fold}.pt\")\n",
        "                else:\n",
        "                  early_stop_counter +=1\n",
        "                  if early_stop_counter > early_stop_patience:\n",
        "                    break\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "            accuracy, auroc = classification_scores(model, testloader, device, opt.task)\n",
        "            print('TEST ACCURACY: %.3f' % accuracy)\n",
        "            best_valid_accuracy_list.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# End K Fold\n",
        "# Calculate the average of the best accuracy from each fold\n",
        "average_best_valid_accuracy = sum(best_valid_accuracy_list) / len(best_valid_accuracy_list)\n",
        "print('Average best validation accuracy from all folds:', average_best_valid_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DtK60sSdTjb",
        "outputId": "7c72fc10-7f27-4a1a-a4d1-3cacd87696ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array(62.33766, dtype=float32),\n",
              " array(62.33766, dtype=float32),\n",
              " array(62.33766, dtype=float32),\n",
              " array(62.33766, dtype=float32)]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_valid_accuracy_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcKdz6R4dUyU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
