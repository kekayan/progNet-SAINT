{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from utils import count_parameters, classification_scores, mean_sq_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from models import SAINT\n",
    "from augmentations import embed_data_mask\n",
    "from augmentations import add_noise\n",
    "from pretraining import SAINT_pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clinical_and_other_features.csv\")\n",
    "df2 = pd.read_csv('../data/clinical_and_other_features_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dict = {\n",
    "    'd_task': 'clf',\n",
    "    'dtask': 'clf',\n",
    "    'task': 'multiclass',\n",
    "    'batchsize': 16,\n",
    "    'pt_aug': ['mixup', 'cutmix'],\n",
    "    'pt_aug_lam': 0.1,\n",
    "    'pretrain_epochs': 250, #50\n",
    "    'nce_temp': 0.7,\n",
    "    'lam0': 0.5,\n",
    "    'lam1': 10,\n",
    "    'lam2': 1,\n",
    "    'lam3': 10,\n",
    "    'pt_projhead_style': 'diff',\n",
    "    'pt_tasks': ['contrastive','denoising'],\n",
    "    'mixup_lam': 0.3,\n",
    "    'ssl_samples': 312,\n",
    "    'lr':0.0001,\n",
    "    'train_noise_type':None,\n",
    "    'train_noise_level':0,\n",
    "    'save_path':\"./output/model.pt\"\n",
    "}\n",
    "\n",
    "class AttributeDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "opt = AttributeDict(opt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X,y,nan_mask,indices):\n",
    "    x_d = {\n",
    "        'data': X.values[indices],\n",
    "        'mask': nan_mask.values[indices]\n",
    "    }\n",
    "\n",
    "    if x_d['data'].shape != x_d['mask'].shape:\n",
    "        raise'Shape of data not same as that of nan mask!'\n",
    "\n",
    "    y_d = {\n",
    "        'data': y[indices].reshape(-1, 1)\n",
    "    }\n",
    "    return x_d, y_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetCatCon(Dataset):\n",
    "    def __init__(self, X, Y, cat_cols,task='clf',continuous_mean_std=None):\n",
    "\n",
    "        cat_cols = list(cat_cols)\n",
    "        X_mask =  X['mask'].copy()\n",
    "        X = X['data'].copy()\n",
    "        con_cols = list(set(np.arange(X.shape[1])) - set(cat_cols))\n",
    "        self.X1 = X[:,cat_cols].copy().astype(np.int64) #categorical columns\n",
    "        self.X2 = X[:,con_cols].copy().astype(np.float32) #numerical columns\n",
    "        self.X1_mask = X_mask[:,cat_cols].copy().astype(np.int64) #categorical columns\n",
    "        self.X2_mask = X_mask[:,con_cols].copy().astype(np.int64) #numerical columns\n",
    "        self.y = Y['data']#.astype(np.float32) if regression\n",
    "        self.cls = np.zeros_like(self.y,dtype=int)\n",
    "        self.cls_mask = np.ones_like(self.y,dtype=int)\n",
    "        if continuous_mean_std is not None:\n",
    "            mean, std = continuous_mean_std\n",
    "            self.X2 = (self.X2 - mean) / std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # X1 has categorical data, X2 has continuous\n",
    "        return np.concatenate((self.cls[idx], self.X1[idx])), self.X2[idx],self.y[idx], np.concatenate((self.cls_mask[idx], self.X1_mask[idx])), self.X2_mask[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df,p=[.65, .15, .2]):\n",
    "  df1 = df.drop(['Overall Near-complete Response:  Looser Definition','Near-complete Response (Graded Measure)'],axis=1)\n",
    "  df1.columns = df1.columns.str.strip()\n",
    "  pathologic_response_to_neoadjuvant_therapy = ['Pathologic response to Neoadjuvant therapy: Pathologic stage (T) following neoadjuvant therapy',\n",
    "        'Pathologic response to Neoadjuvant therapy:  Pathologic stage (N) following neoadjuvant therapy',\n",
    "        'Pathologic response to Neoadjuvant therapy:  Pathologic stage (M) following neoadjuvant therapy']\n",
    "  # df1.drop(pathologic_response_to_neoadjuvant_therapy, axis=1, inplace=True)\n",
    "  X = df1.drop('Overall Near-complete Response:  Stricter Definition',axis=1)\n",
    "  y = df1['Overall Near-complete Response:  Stricter Definition']\n",
    "  cont_columns = ['Date of Birth (Days)', 'Days to Surgery (from the date of diagnosis)', 'Age at last contact in EMR f/u(days)(from the date of diagnosis) ,last time patient known to be alive, unless age of death is reported(in such case the age of death',\n",
    "    'Age at mammo (days)', 'Days to distant recurrence(from the date of diagnosis)', 'Days to local recurrence (from the date of diagnosis)',\n",
    "    'Days to death (from the date of diagnosis)', 'Days to last local recurrence free assessment (from the date of diagnosis)',\n",
    "    ]\n",
    "  categorical_columns = list(set(X.columns) - set(cont_columns))\n",
    "\n",
    "  # convert categorical columns to str type\n",
    "  X[categorical_columns] = X[categorical_columns].astype(str)\n",
    "\n",
    "  cat_idxs = [X.columns.get_loc(c) for c in categorical_columns]\n",
    "  con_idxs = [X.columns.get_loc(c) for c in cont_columns]\n",
    "  X[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p = [.65, .15, .2], size=(X.shape[0],))\n",
    "\n",
    "  train_indices = X[X.Set==\"train\"].index\n",
    "  valid_indices = X[X.Set==\"valid\"].index\n",
    "  test_indices = X[X.Set==\"test\"].index\n",
    "\n",
    "  X = X.drop(columns=['Set'])\n",
    "  temp = X.fillna(\"MissingValue\")\n",
    "#   creates a bert style mask for the missing values\n",
    "  nan_mask = temp.ne(\"MissingValue\").astype(int)\n",
    "\n",
    "  cat_dims = []\n",
    "  for col in categorical_columns:\n",
    "      X[col] = X[col].fillna(\"MissingValue\")\n",
    "      l_enc = LabelEncoder()\n",
    "      X[col] = l_enc.fit_transform(X[col].values)\n",
    "      cat_dims.append(len(l_enc.classes_))\n",
    "\n",
    "  for col in cont_columns:\n",
    "      X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "      X.fillna(X.loc[train_indices, col].mean(), inplace=True)\n",
    "  y = y.values\n",
    "  l_enc = LabelEncoder()\n",
    "  y = l_enc.fit_transform(y)\n",
    "  X_train, y_train = data_split(X,y,nan_mask,train_indices)\n",
    "  X_valid, y_valid = data_split(X,y,nan_mask,valid_indices)\n",
    "  X_test, y_test = data_split(X,y,nan_mask,test_indices)\n",
    "  train_mean, train_std = np.array(X_train['data'][:,con_idxs],dtype=np.float32).mean(0), np.array(X_train['data'][:,con_idxs],dtype=np.float32).std(0)\n",
    "  train_std = np.where(train_std < 1e-6, 1e-6, train_std)\n",
    "  continuous_mean_std = np.array([train_mean,train_std]).astype(np.float32)\n",
    "  train_ds = DataSetCatCon(X_train, y_train, cat_idxs,'clf',continuous_mean_std)\n",
    "  trainloader = DataLoader(train_ds, batch_size=64, shuffle=True,num_workers=1)\n",
    "\n",
    "  valid_ds = DataSetCatCon(X_valid, y_valid, cat_idxs,'clf', continuous_mean_std)\n",
    "  validloader = DataLoader(valid_ds, batch_size=64, shuffle=False,num_workers=1)\n",
    "\n",
    "  test_ds = DataSetCatCon(X_test, y_test, cat_idxs,'clf', continuous_mean_std)\n",
    "  testloader = DataLoader(test_ds, batch_size=64, shuffle=False,num_workers=1)\n",
    "  y_dim = len(np.unique(y_train['data'][:,0]))\n",
    "  print('Number of classes in train:',y_dim)\n",
    "  # in test\n",
    "  print('Number of classes in test:',len(np.unique(y_test['data'][:,0]))\n",
    "  )\n",
    "  #in valid\n",
    "  print('Number of classes in valid:',len(np.unique(y_valid['data'][:,0]))\n",
    "  )\n",
    "\n",
    "\n",
    "  cat_dims = np.append(np.array([1]),np.array(cat_dims)).astype(int) #Appending 1 for CLS token, this is later used to generate embeddings.\n",
    "\n",
    "  return trainloader, validloader, testloader, cat_dims, con_idxs , cat_idxs, y_dim , continuous_mean_std , X_train, y_train, X_valid, y_valid, X_test, y_test, train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in train: 5\n",
      "Number of classes in test: 5\n",
      "Number of classes in valid: 5\n"
     ]
    }
   ],
   "source": [
    "trainloader, validloader, testloader, cat_dims, con_idxs , cat_idxs, y_dim , continuous_mean_std, X_train, y_train, X_valid, y_valid, X_test, y_test, train_ds, valid_ds = prepare_dataset(df,[.8, .2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAINT(\n",
       "  (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "  (simple_MLP): ModuleList(\n",
       "    (0-7): 8 x simple_MLP(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=100, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=100, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer): RowColTransformer(\n",
       "    (embeds): Embedding(1518, 8)\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((656,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=656, out_features=768, bias=False)\n",
       "              (to_out): Linear(in_features=256, out_features=656, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((656,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=656, out_features=5248, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.8, inplace=False)\n",
       "                (3): Linear(in_features=2624, out_features=656, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mask_embed): Embedding(82, 8)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=656, out_features=328, bias=True)\n",
       "      (1): Linear(in_features=328, out_features=164, bias=True)\n",
       "      (2): Linear(in_features=164, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (embeds): Embedding(1518, 8)\n",
       "  (mask_embeds_cat): Embedding(148, 8)\n",
       "  (mask_embeds_cont): Embedding(16, 8)\n",
       "  (single_mask): Embedding(2, 8)\n",
       "  (pos_encodings): Embedding(82, 8)\n",
       "  (mlp1): sep_MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=14, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (11): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=36, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (12-13): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (14): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (15-16): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (17): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (18): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (19): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (20-21): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (22): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (23): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (24-25): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=10, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (26): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (27): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (28): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (29-30): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (31-32): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (33): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (34): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (35): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=36, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (36): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=9, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (37): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=7, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (38): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (39): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (40): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (41): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (42): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (43): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (44): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (45): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (46): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (47): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (48): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (49): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (50): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (51): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (52): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (53): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (54): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (55): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (56): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (57): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=261, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (58): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (59): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (60): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (61): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (62): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (63): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (64): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (65): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=18, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (66): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (67): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=723, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (68): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=48, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (69): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=7, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (70): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=14, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (71): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (72-73): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp2): sep_MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlpfory): simple_MLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=1000, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1000, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pt_mlp): simple_MLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=656, out_features=787, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=787, out_features=328, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pt_mlp2): simple_MLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=656, out_features=787, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=787, out_features=328, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAINT(\n",
    "categories = tuple(cat_dims),\n",
    "num_continuous = len(con_idxs),\n",
    "dim = 8,              # embedding dimension\n",
    "dim_out = 1,\n",
    "depth = 1,             # depth of the network (nr. of transformer blocks)\n",
    "heads = 4,             # number of attention heads 8\n",
    "attn_dropout = 0.1,\n",
    "ff_dropout = 0.8,\n",
    "mlp_hidden_mults = (4, 2),\n",
    "cont_embeddings = 'MLP', # options: 'MLP', 'linear', 'hybrid' (MLP with continuous embeddings concatenated to the transformer block outputs)\n",
    "attentiontype = 'row', # options: 'col', 'row', 'colrow', 'colrowv2'\n",
    "final_mlp_style = 'sep',\n",
    "y_dim = y_dim\n",
    ")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining begins!\n",
      "Model Checkpoint Saved!\n",
      "Epoch: 0, Running Loss: 486394.9138183594 , Val Loss: 22833.375\n",
      "Model Checkpoint Saved!\n",
      "Epoch: 1, Running Loss: 407041.81005859375 , Val Loss: 4351.8984375\n",
      "Early Stopping Counter:  1\n",
      "Epoch: 2, Running Loss: 368174.34130859375 , Val Loss: 14736.794921875\n",
      "Early Stopping Counter:  2\n",
      "Epoch: 3, Running Loss: 316348.3505859375 , Val Loss: 10527.404296875\n",
      "Early Stopping Counter:  3\n",
      "Epoch: 4, Running Loss: 260474.99877929688 , Val Loss: 7408.66357421875\n",
      "Early Stopping Counter:  4\n",
      "Epoch: 5, Running Loss: 168812.5994873047 , Val Loss: 12901.056640625\n",
      "Model Checkpoint Saved!\n",
      "Epoch: 6, Running Loss: 66323.42533874512 , Val Loss: 2785.91943359375\n",
      "Model Checkpoint Saved!\n",
      "Epoch: 7, Running Loss: 78672.84365844727 , Val Loss: 555.6526489257812\n",
      "Early Stopping Counter:  1\n",
      "Epoch: 8, Running Loss: 51596.31997680664 , Val Loss: 19528.56640625\n",
      "Early Stopping Counter:  2\n",
      "Epoch: 9, Running Loss: 70233.99475097656 , Val Loss: 13340.3330078125\n",
      "Model Checkpoint Saved!\n",
      "Epoch: 10, Running Loss: 62023.79772949219 , Val Loss: 517.2745361328125\n",
      "Early Stopping Counter:  1\n",
      "Epoch: 11, Running Loss: 51164.59713745117 , Val Loss: 7967.12890625\n",
      "Early Stopping Counter:  2\n",
      "Epoch: 12, Running Loss: 54971.664962768555 , Val Loss: 527.6917114257812\n",
      "Model Checkpoint Saved!\n",
      "Epoch: 13, Running Loss: 49017.793869018555 , Val Loss: 380.2132873535156\n",
      "Early Stopping Counter:  1\n",
      "Epoch: 14, Running Loss: 54441.686767578125 , Val Loss: 6044.916015625\n",
      "Early Stopping Counter:  2\n",
      "Epoch: 15, Running Loss: 45364.6649017334 , Val Loss: 13048.943359375\n",
      "Early Stopping Counter:  3\n",
      "Epoch: 16, Running Loss: 57010.32002258301 , Val Loss: 14042.4873046875\n",
      "Early Stopping Counter:  4\n",
      "Epoch: 17, Running Loss: 55183.91976928711 , Val Loss: 14852.130859375\n",
      "Early Stopping Counter:  5\n",
      "Epoch: 18, Running Loss: 53198.82176208496 , Val Loss: 3440.72314453125\n",
      "Early Stopping Counter:  6\n",
      "Epoch: 19, Running Loss: 54130.302993774414 , Val Loss: 569.8720703125\n",
      "Early Stopping Counter:  7\n",
      "Epoch: 20, Running Loss: 70754.49130249023 , Val Loss: 1853.00732421875\n",
      "Early Stopping Counter:  8\n",
      "Epoch: 21, Running Loss: 47828.68943786621 , Val Loss: 1574.382568359375\n",
      "Early Stopping Counter:  9\n",
      "Epoch: 22, Running Loss: 43143.99020385742 , Val Loss: 3979.5\n",
      "Early Stopping Counter:  10\n",
      "Epoch: 23, Running Loss: 68932.64712524414 , Val Loss: 5691.33056640625\n",
      "Early Stopping Counter:  11\n",
      "Early Stopping!\n",
      "END OF PRETRAINING!\n"
     ]
    }
   ],
   "source": [
    "model = SAINT_pretrain(model,train_ds, valid_ds , opt, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in train: 4\n",
      "Number of classes in test: 4\n",
      "Number of classes in valid: 4\n"
     ]
    }
   ],
   "source": [
    "trainloader, validloader, testloader, cat_dims, con_idxs , cat_idxs, y_dim , continuous_mean_std, X_train, y_train, X_valid, y_valid, X_test, y_test,_,_ = prepare_dataset(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(),lr=0.0001, betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsave_path='outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are in semi-supervised learning case\n"
     ]
    }
   ],
   "source": [
    "print('We are in semi-supervised learning case')\n",
    "\n",
    "train_bsize = min(opt.ssl_samples//4,opt.batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "trainloader = DataLoader(train_ds, batch_size=train_bsize, shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins now for # 0 Fold.\n",
      "15.866596043109894\n",
      "[EPOCH 1] VALID ACCURACY: 59.615\n",
      "save model\n",
      "9.502539575099945\n",
      "9.216988503932953\n",
      "8.862392604351044\n",
      "8.98517221212387\n",
      "8.29756373167038\n",
      "[EPOCH 6] VALID ACCURACY: 59.615\n",
      "8.135764181613922\n",
      "7.597042411565781\n",
      "7.038891047239304\n",
      "6.091987729072571\n",
      "5.425549209117889\n",
      "[EPOCH 11] VALID ACCURACY: 75.000\n",
      "save model\n",
      "5.187266498804092\n",
      "5.14768123626709\n",
      "4.472138300538063\n",
      "4.256547451019287\n",
      "4.8955088108778\n",
      "[EPOCH 16] VALID ACCURACY: 84.615\n",
      "save model\n",
      "3.5341133773326874\n",
      "3.731830283999443\n",
      "2.7514409869909286\n",
      "2.582195222377777\n",
      "2.3576101511716843\n",
      "[EPOCH 21] VALID ACCURACY: 78.846\n",
      "2.6239263713359833\n",
      "2.5860855504870415\n",
      "2.6485532224178314\n",
      "2.9966431632637978\n",
      "1.8103970400989056\n",
      "[EPOCH 26] VALID ACCURACY: 86.538\n",
      "save model\n",
      "2.3052387312054634\n",
      "2.524137370288372\n",
      "2.9214399978518486\n",
      "2.2304787896573544\n",
      "2.583153784275055\n",
      "[EPOCH 31] VALID ACCURACY: 86.538\n",
      "2.372879274189472\n",
      "1.833578396588564\n",
      "2.1592758372426033\n",
      "1.9373781271278858\n",
      "1.83826357498765\n",
      "[EPOCH 36] VALID ACCURACY: 76.923\n",
      "1.35730866715312\n",
      "1.322531059384346\n",
      "1.2564571797847748\n",
      "1.292900213971734\n",
      "1.3069525621831417\n",
      "[EPOCH 41] VALID ACCURACY: 86.538\n",
      "0.9549212194979191\n",
      "1.0447183568030596\n",
      "0.76505276421085\n",
      "0.8208954762667418\n",
      "0.906160939950496\n",
      "[EPOCH 46] VALID ACCURACY: 90.385\n",
      "save model\n",
      "0.8802927881479263\n",
      "0.9826667327433825\n",
      "1.2138361483812332\n",
      "0.862925442866981\n",
      "0.8014988210052252\n",
      "[EPOCH 51] VALID ACCURACY: 86.538\n",
      "0.5023361928761005\n",
      "0.7459851978346705\n",
      "0.6433652047999203\n",
      "0.4551433022134006\n",
      "0.8359814379364252\n",
      "[EPOCH 56] VALID ACCURACY: 92.308\n",
      "save model\n",
      "0.800411987118423\n",
      "0.9115570336580276\n",
      "0.6330115115270019\n",
      "0.46049280650913715\n",
      "0.4406921644695103\n",
      "[EPOCH 61] VALID ACCURACY: 86.538\n",
      "0.4059223039075732\n",
      "0.8531963432906196\n",
      "0.5340856313705444\n",
      "1.0075930808088742\n",
      "1.2327257678844035\n",
      "[EPOCH 66] VALID ACCURACY: 90.385\n",
      "0.9256667043082416\n",
      "0.6412874888628721\n",
      "0.27425020677037537\n",
      "0.5395741448737681\n",
      "0.540469495113939\n",
      "[EPOCH 71] VALID ACCURACY: 88.462\n",
      "0.4416709579527378\n",
      "0.3722894375678152\n",
      "0.30122349481098354\n",
      "0.22737071209121495\n",
      "0.11054865980986506\n",
      "[EPOCH 76] VALID ACCURACY: 90.385\n",
      "0.30396607506554574\n",
      "0.23254569713026285\n",
      "1.1617482599103823\n",
      "0.1366422411520034\n",
      "0.10291666153352708\n",
      "[EPOCH 81] VALID ACCURACY: 92.308\n",
      "0.07713119371328503\n",
      "0.18604626425076276\n",
      "0.23180345608852804\n",
      "0.1554406846407801\n",
      "0.4403545299428515\n",
      "[EPOCH 86] VALID ACCURACY: 84.615\n",
      "0.29316702554933727\n",
      "0.8365953188913409\n",
      "0.18552613584324718\n",
      "0.13536220171954483\n",
      "0.21331785060465336\n",
      "[EPOCH 91] VALID ACCURACY: 92.308\n",
      "0.09377542027505115\n",
      "0.12229809584096074\n",
      "0.21310306224040687\n",
      "0.10273002108442597\n",
      "0.06437294636270963\n",
      "[EPOCH 96] VALID ACCURACY: 90.385\n",
      "0.12979649724729825\n",
      "0.041119410743704066\n",
      "0.09654008477809839\n",
      "0.028014645213261247\n",
      "0.04720071839983575\n",
      "[EPOCH 101] VALID ACCURACY: 92.308\n",
      "0.06405161757720634\n",
      "0.05199611066200305\n",
      "0.07223361227079295\n",
      "0.03357915842207149\n",
      "0.09840664979856228\n",
      "[EPOCH 106] VALID ACCURACY: 88.462\n",
      "0.5840273601352237\n",
      "0.07065775751834735\n",
      "1.096420414673048\n",
      "0.11147452075965703\n",
      "0.07784774655010551\n",
      "[EPOCH 111] VALID ACCURACY: 88.462\n",
      "0.06961799046257511\n",
      "0.23742603650316596\n",
      "0.19579619061551057\n",
      "2.7670124181895517\n",
      "0.25830879784189165\n",
      "[EPOCH 116] VALID ACCURACY: 84.615\n",
      "0.19533381646033376\n",
      "0.2545690678525716\n",
      "0.2471442454843782\n",
      "0.3349219076917507\n",
      "0.18089188379235566\n",
      "[EPOCH 121] VALID ACCURACY: 84.615\n",
      "0.12837992352433503\n",
      "0.04588483803672716\n",
      "2.887311325437622\n",
      "0.25244467286393046\n",
      "0.08489901147549972\n",
      "[EPOCH 126] VALID ACCURACY: 92.308\n",
      "0.03294787113554776\n",
      "0.043793582706712186\n",
      "0.09808480078936554\n",
      "0.017524797080113785\n",
      "0.04399244996602647\n",
      "[EPOCH 131] VALID ACCURACY: 90.385\n",
      "0.023758600349538028\n",
      "0.024427478114375845\n",
      "0.020274114111089148\n",
      "0.027791405547759496\n",
      "0.02118610864272341\n",
      "[EPOCH 136] VALID ACCURACY: 92.308\n",
      "0.014943698115530424\n",
      "0.05545832357893232\n",
      "0.10488390803220682\n",
      "0.03856546391034499\n",
      "0.06694738566875458\n",
      "[EPOCH 141] VALID ACCURACY: 90.385\n",
      "0.040720152348512784\n",
      "0.48692252523505886\n",
      "0.602478088789212\n",
      "0.07914801966398954\n",
      "0.20433966448763385\n",
      "[EPOCH 146] VALID ACCURACY: 90.385\n",
      "0.06678818596992642\n",
      "0.024949808313976973\n",
      "0.02944430845673196\n",
      "2.5094975806423463\n",
      "0.44014348357450217\n",
      "[EPOCH 151] VALID ACCURACY: 88.462\n",
      "0.4631540035479702\n",
      "0.056334331806283444\n",
      "0.11401128725992749\n",
      "0.4055460770614445\n",
      "0.6061896874452941\n",
      "[EPOCH 156] VALID ACCURACY: 86.538\n",
      "0.1296276549983304\n",
      "0.037318750109989196\n",
      "0.07619402988348156\n",
      "0.10495748772518709\n",
      "0.18215925586991943\n",
      "[EPOCH 161] VALID ACCURACY: 88.462\n",
      "TEST ACCURACY: 88.462\n",
      "Training begins now for # 1 Fold.\n",
      "3.245181689504534\n",
      "[EPOCH 1] VALID ACCURACY: 100.000\n",
      "save model\n",
      "1.655624944716692\n",
      "1.3154831556603312\n",
      "0.9481824161484838\n",
      "0.5706760245375335\n",
      "0.7457098527811468\n",
      "[EPOCH 6] VALID ACCURACY: 100.000\n",
      "0.6458841066341847\n",
      "0.7872549435123801\n",
      "0.8272469914518297\n",
      "0.7134978123940527\n",
      "0.38319104816764593\n",
      "[EPOCH 11] VALID ACCURACY: 98.077\n",
      "0.12885766482213512\n",
      "0.12281849491409957\n",
      "0.061005928364465944\n",
      "0.37064061366254464\n",
      "1.645368937010062\n",
      "[EPOCH 16] VALID ACCURACY: 98.077\n",
      "0.5759945064783096\n",
      "0.3305698854383081\n",
      "0.17513321025762707\n",
      "0.2457591407583095\n",
      "0.31845436606090516\n",
      "[EPOCH 21] VALID ACCURACY: 100.000\n",
      "2.5380752072669566\n",
      "0.37528707436285913\n",
      "0.11477599968202412\n",
      "0.1870533877518028\n",
      "0.08677139785140753\n",
      "[EPOCH 26] VALID ACCURACY: 100.000\n",
      "0.11025740054901689\n",
      "0.14540454247617163\n",
      "0.06994770382880233\n",
      "0.017501587804872543\n",
      "0.03612948197405785\n",
      "[EPOCH 31] VALID ACCURACY: 98.077\n",
      "0.08259111261577345\n",
      "0.0375301725580357\n",
      "0.15987649536691606\n",
      "0.01979650987777859\n",
      "0.015341284481110051\n",
      "[EPOCH 36] VALID ACCURACY: 100.000\n",
      "0.05235439476382453\n",
      "0.013879126847314183\n",
      "0.06316872752358904\n",
      "0.011424943353631534\n",
      "0.022512542323966045\n",
      "[EPOCH 41] VALID ACCURACY: 100.000\n",
      "0.012413696720614098\n",
      "0.018681126355659217\n",
      "0.011903726559467032\n",
      "0.014360646571731195\n",
      "0.004174223315203562\n",
      "[EPOCH 46] VALID ACCURACY: 100.000\n",
      "0.0050596920773386955\n",
      "0.010678496400942095\n",
      "0.019689640856086044\n",
      "0.006369649057887727\n",
      "0.010229615611024201\n",
      "[EPOCH 51] VALID ACCURACY: 100.000\n",
      "0.003818408600636758\n",
      "0.003766533831367269\n",
      "0.3668165586714167\n",
      "0.028381125463056378\n",
      "0.02604422789590899\n",
      "[EPOCH 56] VALID ACCURACY: 100.000\n",
      "0.010299019239027984\n",
      "0.008008902303117793\n",
      "0.013701268093427643\n",
      "0.013160327645891812\n",
      "0.006069537586881779\n",
      "[EPOCH 61] VALID ACCURACY: 100.000\n",
      "0.00666302717581857\n",
      "0.005658907990437001\n",
      "0.0026385152523289435\n",
      "0.005071056795713957\n",
      "0.0035460217332001776\n",
      "[EPOCH 66] VALID ACCURACY: 100.000\n",
      "0.007911126092949416\n",
      "0.03820698244817322\n",
      "0.004855252678680699\n",
      "0.014951479042792926\n",
      "0.006782717020541895\n",
      "[EPOCH 71] VALID ACCURACY: 100.000\n",
      "0.006684363142994698\n",
      "0.006736420786182862\n",
      "0.006905195034050848\n",
      "0.004930203838739544\n",
      "0.008943252592871431\n",
      "[EPOCH 76] VALID ACCURACY: 96.154\n",
      "0.003570116719856742\n",
      "0.0062464070397254545\n",
      "0.004887422212050296\n",
      "0.006810931909058127\n",
      "0.005331331882189261\n",
      "[EPOCH 81] VALID ACCURACY: 100.000\n",
      "0.004485357942030532\n",
      "0.0020728081653942354\n",
      "0.004720579836430261\n",
      "0.0029337907617446035\n",
      "0.007245203740239958\n",
      "[EPOCH 86] VALID ACCURACY: 100.000\n",
      "0.0037732498785771895\n",
      "0.0016107630435726605\n",
      "0.010742673048298457\n",
      "0.006294904371316079\n",
      "0.002401898833340965\n",
      "[EPOCH 91] VALID ACCURACY: 100.000\n",
      "0.0016022341205825796\n",
      "0.0023524766038462985\n",
      "0.00502260844950797\n",
      "0.0034030836905003525\n",
      "0.0009506546721240738\n",
      "[EPOCH 96] VALID ACCURACY: 98.077\n",
      "0.004857859021285549\n",
      "0.002550525237893453\n",
      "0.002046186497864255\n",
      "0.009689000340586063\n",
      "0.0014360304339788854\n",
      "[EPOCH 101] VALID ACCURACY: 100.000\n",
      "0.008451270540376754\n",
      "0.00409742125611956\n",
      "0.011530022893566638\n",
      "0.001353044790448621\n",
      "0.005913725416576199\n",
      "[EPOCH 106] VALID ACCURACY: 100.000\n",
      "TEST ACCURACY: 90.385\n",
      "Training begins now for # 2 Fold.\n",
      "0.014677280571049778\n",
      "[EPOCH 1] VALID ACCURACY: 100.000\n",
      "TEST ACCURACY: 88.462\n",
      "Training begins now for # 3 Fold.\n",
      "0.009680556226157933\n",
      "[EPOCH 1] VALID ACCURACY: 100.000\n",
      "TEST ACCURACY: 88.462\n",
      "Average best validation accuracy from all folds: 88.94231033325195\n"
     ]
    }
   ],
   "source": [
    "# Start K-Fold Cross Validation\n",
    "# Define the number of splits\n",
    "n_splits = 4\n",
    "best_valid_auroc = 0\n",
    "best_valid_accuracy = 0\n",
    "best_test_auroc = 0\n",
    "best_test_accuracy = 0\n",
    "best_valid_rmse = 100000\n",
    "\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 20\n",
    "\n",
    "# fold_dict = {}\n",
    "\n",
    "# Define the KFold object\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=21)\n",
    "\n",
    "# Initialize lists to store the train and validation indices for each fold\n",
    "train_indices_list = []\n",
    "valid_indices_list = []\n",
    "\n",
    "# Loop over the splits and get the train and validation indices for each fold\n",
    "for train_indices, valid_indices in kf.split(X_train['data']):\n",
    "    train_indices_list.append(train_indices)\n",
    "    valid_indices_list.append(valid_indices)\n",
    "best_test_accuracy_list = []\n",
    "# Loop over the folds and train the model on each fold\n",
    "for fold in range(n_splits):\n",
    "    # Get the train and validation indices for this fold\n",
    "    train_indices = train_indices_list[fold]\n",
    "    valid_indices = valid_indices_list[fold]\n",
    "\n",
    "    # Create the train and validation datasets and dataloaders for this fold\n",
    "    train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "    trainloader = DataLoader(train_ds, batch_size=train_bsize,num_workers=2, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n",
    "    valid_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "    validloader = DataLoader(valid_ds, batch_size=train_bsize, shuffle=False,num_workers=2, sampler=torch.utils.data.SubsetRandomSampler(valid_indices))\n",
    "    print(f'Training begins now for # {fold} Fold.')\n",
    "    # Train the model on this fold\n",
    "    for epoch in range(300):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            # x_categ is the the categorical data, with y appended as last feature. x_cont has continuous data. cat_mask is an array of ones same shape as x_categ except for last column(corresponding to y's) set to 0s. con_mask is an array of ones same shape as x_cont.\n",
    "            x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "            if opt.train_noise_type is not None and opt.train_noise_level>0:\n",
    "                noise_dict = {\n",
    "                    'noise_type' : opt.train_noise_type,\n",
    "                    'lambda' : opt.train_noise_level\n",
    "                }\n",
    "                if opt.train_noise_type == 'cutmix':\n",
    "                    x_categ, x_cont = add_noise(x_categ,x_cont, noise_params = noise_dict)\n",
    "                elif opt.train_noise_type == 'missing':\n",
    "                    cat_mask, con_mask = add_noise(cat_mask, con_mask, noise_params = noise_dict)\n",
    "            # We are converting the data to embeddings in the next step\n",
    "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model)\n",
    "            reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "            # select only the representations corresponding to y and apply mlp on it in the next step to get the predictions.\n",
    "            y_reps = reps[:,0,:]\n",
    "\n",
    "            y_outs = model.mlpfory(y_reps)\n",
    "            if opt.task == 'regression':\n",
    "                loss = criterion(y_outs,y_gts)\n",
    "            else:\n",
    "                loss = criterion(y_outs,y_gts.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(running_loss)\n",
    "        if epoch%5==0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if opt.task in ['binary','multiclass']:\n",
    "                    accuracy, auroc = classification_scores(model, validloader, device, opt.task)\n",
    "                    # test_accuracy, test_auroc = classification_scores(model, testloader, device, opt.task)\n",
    "\n",
    "                    print('[EPOCH %d] VALID ACCURACY: %.3f' %\n",
    "                        (epoch + 1, accuracy ))\n",
    "                    # print('[EPOCH %d] TEST ACCURACY: %.3f' %\n",
    "                    #     (epoch + 1, test_accuracy ))\n",
    "\n",
    "            if opt.task =='multiclass':\n",
    "                if accuracy > best_valid_accuracy:\n",
    "                    best_valid_accuracy = accuracy\n",
    "                    early_stop_counter = 0\n",
    "                    print(\"save model\")\n",
    "                    torch.save({'model': model, 'state_dict': model.state_dict(),'optimizer' : optimizer.state_dict()},modelsave_path+f\"model-{fold}.pt\")\n",
    "                else:\n",
    "                  early_stop_counter +=1\n",
    "                  if early_stop_counter > early_stop_patience:\n",
    "                    break\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            accuracy, auroc = classification_scores(model, testloader, device, opt.task)\n",
    "            print('TEST ACCURACY: %.3f' % accuracy)\n",
    "            best_test_accuracy_list.append(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# End K Fold\n",
    "# Calculate the average of the best accuracy from each fold\n",
    "average_best_valid_accuracy = sum(best_test_accuracy_list) / len(best_test_accuracy_list)\n",
    "print('Average best validation accuracy from all folds:', average_best_valid_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAINT(\n",
       "  (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "  (simple_MLP): ModuleList(\n",
       "    (0-7): 8 x simple_MLP(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=100, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=100, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer): RowColTransformer(\n",
       "    (embeds): Embedding(828, 8)\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((656,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=656, out_features=768, bias=False)\n",
       "              (to_out): Linear(in_features=256, out_features=656, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((656,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=656, out_features=5248, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.8, inplace=False)\n",
       "                (3): Linear(in_features=2624, out_features=656, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mask_embed): Embedding(82, 8)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=656, out_features=328, bias=True)\n",
       "      (1): Linear(in_features=328, out_features=164, bias=True)\n",
       "      (2): Linear(in_features=164, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (embeds): Embedding(828, 8)\n",
       "  (mask_embeds_cat): Embedding(148, 8)\n",
       "  (mask_embeds_cont): Embedding(16, 8)\n",
       "  (single_mask): Embedding(2, 8)\n",
       "  (pos_encodings): Embedding(82, 8)\n",
       "  (mlp1): sep_MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=12, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (11): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=27, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (12-13): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (14): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (15-18): 4 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (19): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=40, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (20): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (21): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (22): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (23): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (24-25): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=9, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (26): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (27): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (28-30): 3 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (31): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (32): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (33): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (34): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=11, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (35): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=27, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (36): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=9, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (37): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=7, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (38): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (39): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (40-41): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (42): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (43): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (44): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (45): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (46): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (47): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (48): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (49): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (50): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (51): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (52): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (53): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (54): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=40, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (55): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (56): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (57): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=115, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (58-59): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (60): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (61): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (62): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (63): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (64): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (65): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=11, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (66): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (67): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=275, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (68): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=27, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (69): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (70): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=12, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (71): simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (72-73): 2 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp2): sep_MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x simple_MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=8, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlpfory): simple_MLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=1000, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1000, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pt_mlp): simple_MLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=656, out_features=787, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=787, out_features=328, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pt_mlp2): simple_MLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=656, out_features=787, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=787, out_features=328, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAINT(\n",
    "categories = tuple(cat_dims),\n",
    "num_continuous = len(con_idxs),\n",
    "dim = 8,              # embedding dimension\n",
    "dim_out = 1,\n",
    "depth = 1,             # depth of the network (nr. of transformer blocks)\n",
    "heads = 4,             # number of attention heads 8\n",
    "attn_dropout = 0.1,\n",
    "ff_dropout = 0.8,\n",
    "mlp_hidden_mults = (4, 2),\n",
    "cont_embeddings = 'MLP', # options: 'MLP', 'linear', 'hybrid' (MLP with continuous embeddings concatenated to the transformer block outputs)\n",
    "attentiontype = 'row', # options: 'col', 'row', 'colrow', 'colrowv2'\n",
    "final_mlp_style = 'sep',\n",
    "y_dim = y_dim\n",
    ")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins now for # 0 Fold.\n",
      "12.931325435638428\n",
      "[EPOCH 1] VALID ACCURACY: 59.615\n",
      "save model\n",
      "12.966737389564514\n",
      "13.037555932998657\n",
      "13.052281737327576\n",
      "13.073012590408325\n",
      "13.054601550102234\n",
      "[EPOCH 6] VALID ACCURACY: 59.615\n",
      "13.04453194141388\n",
      "12.97135615348816\n",
      "13.053110003471375\n",
      "13.015082001686096\n",
      "13.092105746269226\n",
      "[EPOCH 11] VALID ACCURACY: 59.615\n",
      "13.125950932502747\n",
      "13.035520434379578\n",
      "13.053758978843689\n",
      "13.085068583488464\n",
      "13.03072726726532\n",
      "[EPOCH 16] VALID ACCURACY: 57.692\n",
      "12.95281457901001\n",
      "13.056073069572449\n",
      "12.982487678527832\n",
      "12.989891171455383\n",
      "13.040847301483154\n",
      "[EPOCH 21] VALID ACCURACY: 59.615\n",
      "13.074813842773438\n",
      "13.012911796569824\n",
      "13.027708053588867\n",
      "13.035239458084106\n",
      "12.996838092803955\n",
      "[EPOCH 26] VALID ACCURACY: 59.615\n",
      "13.049815654754639\n",
      "13.002563118934631\n",
      "13.037336826324463\n",
      "12.996376037597656\n",
      "13.089592933654785\n",
      "[EPOCH 31] VALID ACCURACY: 57.692\n",
      "13.066810965538025\n",
      "13.017314195632935\n",
      "12.999440670013428\n",
      "12.976470947265625\n",
      "13.008018732070923\n",
      "[EPOCH 36] VALID ACCURACY: 59.615\n",
      "13.026143193244934\n",
      "13.100628972053528\n",
      "12.97518265247345\n",
      "12.95853590965271\n",
      "13.113233089447021\n",
      "[EPOCH 41] VALID ACCURACY: 59.615\n",
      "12.97753393650055\n",
      "12.898231506347656\n",
      "12.991617798805237\n",
      "13.062005281448364\n",
      "13.047240853309631\n",
      "[EPOCH 46] VALID ACCURACY: 59.615\n",
      "13.06537675857544\n",
      "13.029022336006165\n",
      "13.012576460838318\n",
      "13.039965391159058\n",
      "13.014662623405457\n",
      "[EPOCH 51] VALID ACCURACY: 59.615\n",
      "13.007579326629639\n",
      "12.982535362243652\n",
      "12.997931003570557\n",
      "13.031630158424377\n",
      "12.95499336719513\n",
      "[EPOCH 56] VALID ACCURACY: 57.692\n",
      "12.971017360687256\n",
      "13.012131214141846\n",
      "13.119168877601624\n",
      "12.9461110830307\n",
      "12.971391439437866\n",
      "[EPOCH 61] VALID ACCURACY: 59.615\n",
      "13.013897061347961\n",
      "13.023275375366211\n",
      "13.01976728439331\n",
      "13.076645255088806\n",
      "13.043972253799438\n",
      "[EPOCH 66] VALID ACCURACY: 59.615\n",
      "12.987618088722229\n",
      "13.059797048568726\n",
      "13.011058688163757\n",
      "13.050837755203247\n",
      "13.040085673332214\n",
      "[EPOCH 71] VALID ACCURACY: 59.615\n",
      "12.943788170814514\n",
      "12.952420711517334\n",
      "13.027705192565918\n",
      "12.979004979133606\n",
      "13.066132426261902\n",
      "[EPOCH 76] VALID ACCURACY: 59.615\n",
      "12.99371337890625\n",
      "13.031429767608643\n",
      "13.001144289970398\n",
      "12.977494835853577\n",
      "13.075571537017822\n",
      "[EPOCH 81] VALID ACCURACY: 59.615\n",
      "12.947383284568787\n",
      "12.91734766960144\n",
      "12.994974255561829\n",
      "13.04644227027893\n",
      "13.113256573677063\n",
      "[EPOCH 86] VALID ACCURACY: 57.692\n",
      "13.034124255180359\n",
      "12.971341609954834\n",
      "13.019149422645569\n",
      "12.953888177871704\n",
      "13.092651724815369\n",
      "[EPOCH 91] VALID ACCURACY: 59.615\n",
      "13.020104050636292\n",
      "13.089507818222046\n",
      "12.950026273727417\n",
      "12.944017767906189\n",
      "12.940614461898804\n",
      "[EPOCH 96] VALID ACCURACY: 57.692\n",
      "13.017008185386658\n",
      "13.03338098526001\n",
      "13.00870656967163\n",
      "13.051886200904846\n",
      "13.086230397224426\n",
      "[EPOCH 101] VALID ACCURACY: 59.615\n",
      "12.995999574661255\n",
      "12.972101926803589\n",
      "13.027052402496338\n",
      "13.070956826210022\n",
      "13.133062481880188\n",
      "[EPOCH 106] VALID ACCURACY: 59.615\n",
      "TEST ACCURACY: 67.308\n",
      "Training begins now for # 1 Fold.\n",
      "13.225311279296875\n",
      "[EPOCH 1] VALID ACCURACY: 71.154\n",
      "save model\n",
      "13.256587147712708\n",
      "13.217649459838867\n",
      "13.25267231464386\n",
      "13.187474370002747\n",
      "13.134949684143066\n",
      "[EPOCH 6] VALID ACCURACY: 71.154\n",
      "13.24393343925476\n",
      "13.228179216384888\n",
      "13.203429222106934\n",
      "13.282294392585754\n",
      "13.217705607414246\n",
      "[EPOCH 11] VALID ACCURACY: 73.077\n",
      "save model\n",
      "13.14126706123352\n",
      "13.285662055015564\n",
      "13.186840057373047\n",
      "13.192871451377869\n",
      "13.301267981529236\n",
      "[EPOCH 16] VALID ACCURACY: 73.077\n",
      "13.234981298446655\n",
      "13.209908485412598\n",
      "13.297905325889587\n",
      "13.174185276031494\n",
      "13.155913710594177\n",
      "[EPOCH 21] VALID ACCURACY: 69.231\n",
      "13.18651533126831\n",
      "13.200676918029785\n",
      "13.176816821098328\n",
      "13.194675087928772\n",
      "13.308703660964966\n",
      "[EPOCH 26] VALID ACCURACY: 73.077\n",
      "13.206104755401611\n",
      "13.23014235496521\n",
      "13.220589399337769\n",
      "13.207520127296448\n",
      "13.197508454322815\n",
      "[EPOCH 31] VALID ACCURACY: 73.077\n",
      "13.196528911590576\n",
      "13.226594090461731\n",
      "13.22183632850647\n",
      "13.235055327415466\n",
      "13.22745132446289\n",
      "[EPOCH 36] VALID ACCURACY: 71.154\n",
      "13.212902665138245\n",
      "13.256913423538208\n",
      "13.236164212226868\n",
      "13.20027768611908\n",
      "13.20776641368866\n",
      "[EPOCH 41] VALID ACCURACY: 69.231\n",
      "13.259103059768677\n",
      "13.280091404914856\n",
      "13.186854362487793\n",
      "13.121952295303345\n",
      "13.254876017570496\n",
      "[EPOCH 46] VALID ACCURACY: 73.077\n",
      "13.232884883880615\n",
      "13.245081305503845\n",
      "13.234281063079834\n",
      "13.139702677726746\n",
      "13.150189399719238\n",
      "[EPOCH 51] VALID ACCURACY: 73.077\n",
      "13.138871192932129\n",
      "13.194638013839722\n",
      "13.155365228652954\n",
      "13.302046537399292\n",
      "13.208369731903076\n",
      "[EPOCH 56] VALID ACCURACY: 73.077\n",
      "13.192550420761108\n",
      "13.249829769134521\n",
      "13.211051344871521\n",
      "13.244487643241882\n",
      "13.275640606880188\n",
      "[EPOCH 61] VALID ACCURACY: 71.154\n",
      "13.175381898880005\n",
      "13.256644487380981\n",
      "13.141302347183228\n",
      "13.170261859893799\n",
      "13.205362915992737\n",
      "[EPOCH 66] VALID ACCURACY: 73.077\n",
      "13.161623477935791\n",
      "13.18585479259491\n",
      "13.19124448299408\n",
      "13.218379020690918\n",
      "13.175765991210938\n",
      "[EPOCH 71] VALID ACCURACY: 73.077\n",
      "13.181226134300232\n",
      "13.16574513912201\n",
      "13.200247168540955\n",
      "13.146397948265076\n",
      "13.15149188041687\n",
      "[EPOCH 76] VALID ACCURACY: 71.154\n",
      "13.150984644889832\n",
      "13.18608021736145\n",
      "13.245612382888794\n",
      "13.184004783630371\n",
      "13.238574385643005\n",
      "[EPOCH 81] VALID ACCURACY: 71.154\n",
      "13.213993906974792\n",
      "13.175906300544739\n",
      "13.236020803451538\n",
      "13.169853210449219\n",
      "13.23502504825592\n",
      "[EPOCH 86] VALID ACCURACY: 73.077\n",
      "13.156953692436218\n",
      "13.238494515419006\n",
      "13.222092747688293\n",
      "13.128543734550476\n",
      "13.218822717666626\n",
      "[EPOCH 91] VALID ACCURACY: 69.231\n",
      "13.167724370956421\n",
      "13.195045232772827\n",
      "13.195831418037415\n",
      "13.22609007358551\n",
      "13.164431810379028\n",
      "[EPOCH 96] VALID ACCURACY: 71.154\n",
      "13.325099229812622\n",
      "13.190887570381165\n",
      "13.266597628593445\n",
      "13.18689751625061\n",
      "13.258637428283691\n",
      "[EPOCH 101] VALID ACCURACY: 71.154\n",
      "13.172006607055664\n",
      "13.14329731464386\n",
      "13.221936106681824\n",
      "13.22916316986084\n",
      "13.23630440235138\n",
      "[EPOCH 106] VALID ACCURACY: 71.154\n",
      "13.195394396781921\n",
      "13.165907382965088\n",
      "13.159305214881897\n",
      "13.201449275016785\n",
      "13.175243496894836\n",
      "[EPOCH 111] VALID ACCURACY: 71.154\n",
      "13.196022987365723\n",
      "13.232747197151184\n",
      "13.262452244758606\n",
      "13.232956171035767\n",
      "13.204096555709839\n",
      "[EPOCH 116] VALID ACCURACY: 71.154\n",
      "TEST ACCURACY: 67.308\n",
      "Training begins now for # 2 Fold.\n",
      "13.10874092578888\n",
      "[EPOCH 1] VALID ACCURACY: 63.462\n",
      "TEST ACCURACY: 67.308\n",
      "Training begins now for # 3 Fold.\n",
      "13.130797743797302\n",
      "[EPOCH 1] VALID ACCURACY: 64.706\n",
      "TEST ACCURACY: 67.308\n",
      "Average best validation accuracy from all folds: 67.30769348144531\n"
     ]
    }
   ],
   "source": [
    "# Start K-Fold for without pre-train\n",
    "# Define the number of splits\n",
    "n_splits = 4\n",
    "best_valid_auroc = 0\n",
    "best_valid_accuracy = 0\n",
    "best_test_auroc = 0\n",
    "best_test_accuracy = 0\n",
    "best_valid_rmse = 100000\n",
    "\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 20\n",
    "\n",
    "# fold_dict = {}\n",
    "\n",
    "# Define the KFold object\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=21)\n",
    "\n",
    "# Initialize lists to store the train and validation indices for each fold\n",
    "train_indices_list = []\n",
    "valid_indices_list = []\n",
    "\n",
    "# Loop over the splits and get the train and validation indices for each fold\n",
    "for train_indices, valid_indices in kf.split(X_train['data']):\n",
    "    train_indices_list.append(train_indices)\n",
    "    valid_indices_list.append(valid_indices)\n",
    "best_valid_accuracy_list = []\n",
    "# Loop over the folds and train the model on each fold\n",
    "for fold in range(n_splits):\n",
    "    # Get the train and validation indices for this fold\n",
    "    train_indices = train_indices_list[fold]\n",
    "    valid_indices = valid_indices_list[fold]\n",
    "\n",
    "    # Create the train and validation datasets and dataloaders for this fold\n",
    "    train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "    trainloader = DataLoader(train_ds, batch_size=train_bsize,num_workers=2, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n",
    "    valid_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "    validloader = DataLoader(valid_ds, batch_size=train_bsize, shuffle=False,num_workers=2, sampler=torch.utils.data.SubsetRandomSampler(valid_indices))\n",
    "    print(f'Training begins now for # {fold} Fold.')\n",
    "    # Train the model on this fold\n",
    "    for epoch in range(300):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            # x_categ is the the categorical data, with y appended as last feature. x_cont has continuous data. cat_mask is an array of ones same shape as x_categ except for last column(corresponding to y's) set to 0s. con_mask is an array of ones same shape as x_cont.\n",
    "            x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "            if opt.train_noise_type is not None and opt.train_noise_level>0:\n",
    "                noise_dict = {\n",
    "                    'noise_type' : opt.train_noise_type,\n",
    "                    'lambda' : opt.train_noise_level\n",
    "                }\n",
    "                if opt.train_noise_type == 'cutmix':\n",
    "                    x_categ, x_cont = add_noise(x_categ,x_cont, noise_params = noise_dict)\n",
    "                elif opt.train_noise_type == 'missing':\n",
    "                    cat_mask, con_mask = add_noise(cat_mask, con_mask, noise_params = noise_dict)\n",
    "            # We are converting the data to embeddings in the next step\n",
    "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model)\n",
    "            reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "            # select only the representations corresponding to y and apply mlp on it in the next step to get the predictions.\n",
    "            y_reps = reps[:,0,:]\n",
    "\n",
    "            y_outs = model.mlpfory(y_reps)\n",
    "            if opt.task == 'regression':\n",
    "                loss = criterion(y_outs,y_gts)\n",
    "            else:\n",
    "                loss = criterion(y_outs,y_gts.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(running_loss)\n",
    "        if epoch%5==0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if opt.task in ['binary','multiclass']:\n",
    "                    accuracy, auroc = classification_scores(model, validloader, device, opt.task)\n",
    "                    # test_accuracy, test_auroc = classification_scores(model, testloader, device, opt.task)\n",
    "\n",
    "                    print('[EPOCH %d] VALID ACCURACY: %.3f' %\n",
    "                        (epoch + 1, accuracy ))\n",
    "                    # print('[EPOCH %d] TEST ACCURACY: %.3f' %\n",
    "                    #     (epoch + 1, test_accuracy ))\n",
    "\n",
    "            if opt.task =='multiclass':\n",
    "                if accuracy > best_valid_accuracy:\n",
    "                    best_valid_accuracy = accuracy\n",
    "                    early_stop_counter = 0\n",
    "                    print(\"save model\")\n",
    "                    torch.save({'model': model, 'state_dict': model.state_dict(),'optimizer' : optimizer.state_dict()},modelsave_path+f\"model-{fold}.pt\")\n",
    "                else:\n",
    "                  early_stop_counter +=1\n",
    "                  if early_stop_counter > early_stop_patience:\n",
    "                    break\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            accuracy, auroc = classification_scores(model, testloader, device, opt.task)\n",
    "            print('TEST ACCURACY: %.3f' % accuracy)\n",
    "            best_valid_accuracy_list.append(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# End K Fold\n",
    "# Calculate the average of the best accuracy from each fold\n",
    "average_best_valid_accuracy = sum(best_valid_accuracy_list) / len(best_valid_accuracy_list)\n",
    "print('Average best validation accuracy from all folds:', average_best_valid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
